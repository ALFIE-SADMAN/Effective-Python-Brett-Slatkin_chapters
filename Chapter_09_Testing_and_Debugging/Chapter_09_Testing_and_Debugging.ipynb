{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Testing and Debugging\n",
    "\n",
    "---\n",
    "\n",
    "Good tests are an insurance policy on your code. They give you confidence that your code is correct and make refactoring easier. This chapter covers essential testing and debugging practices in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 75: Use repr Strings for Debugging Output\n",
    "\n",
    "### The Problem with print\n",
    "\n",
    "When debugging, `print()` outputs a human-readable string but doesn't always show the actual type and composition of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic printing - type information is hidden\n",
    "print(5)\n",
    "print('5')\n",
    "\n",
    "int_value = 5\n",
    "str_value = '5'\n",
    "print(f'{int_value} == {str_value} ?')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The repr Solution\n",
    "\n",
    "The `repr()` function returns the **printable representation** of an object, making type differences clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Using repr to see actual types\n",
    "print(repr(5))\n",
    "print(repr('5'))\n",
    "\n",
    "# Shows the difference clearly\n",
    "int_value = 5\n",
    "str_value = '5'\n",
    "print(f'{int_value!r} != {str_value!r}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# repr shows special characters clearly\n",
    "a = '\\x07'  # Bell character\n",
    "print(repr(a))\n",
    "\n",
    "# Newlines and tabs are also visible\n",
    "b = 'Hello\\nWorld\\tTab'\n",
    "print('Normal print:', b)\n",
    "print('With repr:', repr(b))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom __repr__ for Classes\n",
    "\n",
    "Define `__repr__` to make your objects more debuggable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Without custom __repr__\n",
    "class OpaqueClass:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "obj = OpaqueClass(1, 'foo')\n",
    "print(obj)  # Not very helpful"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# With custom __repr__\n",
    "class BetterClass:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'BetterClass({self.x!r}, {self.y!r})'\n",
    "\n",
    "obj = BetterClass(2, 'bar')\n",
    "print(obj)  # Much more informative!"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Using __dict__ when you can't modify the class\n",
    "obj = OpaqueClass(4, 'baz')\n",
    "print(obj.__dict__)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Debugging a data structure\n",
    "data = {\n",
    "    'name': 'Alice',\n",
    "    'age': 30,\n",
    "    'city': 'NYC\\n',  # Hidden newline!\n",
    "    'score': '95'     # String, not int!\n",
    "}\n",
    "\n",
    "print('Normal print:')\n",
    "for key, value in data.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "print('\\nWith repr:')\n",
    "for key, value in data.items():\n",
    "    print(f'{key}: {value!r}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 76: Verify Related Behaviors in TestCase Subclasses\n",
    "\n",
    "### Basic Testing with unittest\n",
    "\n",
    "The `unittest` module provides a framework for writing and running tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a simple function to test\n",
    "def to_str(data):\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    elif isinstance(data, bytes):\n",
    "        return data.decode('utf-8')\n",
    "    else:\n",
    "        raise TypeError('Must supply str or bytes, '\n",
    "                       f'found: {data!r}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from unittest import TestCase, main\n",
    "\n",
    "class UtilsTestCase(TestCase):\n",
    "    def test_to_str_bytes(self):\n",
    "        self.assertEqual('hello', to_str(b'hello'))\n",
    "    \n",
    "    def test_to_str_str(self):\n",
    "        self.assertEqual('hello', to_str('hello'))\n",
    "    \n",
    "    def test_to_str_bad(self):\n",
    "        with self.assertRaises(TypeError):\n",
    "            to_str(object())\n",
    "\n",
    "# Run tests in Jupyter\n",
    "import sys\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(UtilsTestCase)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(suite)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestCase Helper Methods\n",
    "\n",
    "TestCase provides many assertion methods that give better error messages than plain `assert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AssertionExamples(TestCase):\n",
    "    def test_assert_equal(self):\n",
    "        \"\"\"assertEqual shows both values on failure\"\"\"\n",
    "        expected = 12\n",
    "        found = 2 * 5\n",
    "        # This will fail and show: AssertionError: 12 != 10\n",
    "        # self.assertEqual(expected, found)\n",
    "        pass  # Commented out to avoid test failure\n",
    "    \n",
    "    def test_assert_true(self):\n",
    "        \"\"\"assertTrue for boolean conditions\"\"\"\n",
    "        self.assertTrue(5 > 3)\n",
    "        self.assertTrue('hello'.startswith('hel'))\n",
    "    \n",
    "    def test_assert_in(self):\n",
    "        \"\"\"assertIn for membership testing\"\"\"\n",
    "        self.assertIn('a', 'abc')\n",
    "        self.assertIn(2, [1, 2, 3])\n",
    "    \n",
    "    def test_assert_is_instance(self):\n",
    "        \"\"\"assertIsInstance for type checking\"\"\"\n",
    "        self.assertIsInstance(42, int)\n",
    "        self.assertIsInstance('hello', str)\n",
    "\n",
    "# Run the examples\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(AssertionExamples)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(suite)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Test Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def sum_squares(values):\n",
    "    \"\"\"Generate cumulative sum of squares\"\"\"\n",
    "    cumulative = 0\n",
    "    for value in values:\n",
    "        cumulative += value ** 2\n",
    "        yield cumulative\n",
    "\n",
    "class HelperTestCase(TestCase):\n",
    "    def verify_sum_squares(self, values, expected):\n",
    "        \"\"\"Custom helper for testing generators\"\"\"\n",
    "        expect_it = iter(expected)\n",
    "        found_it = iter(sum_squares(values))\n",
    "        test_it = zip(expect_it, found_it)\n",
    "        \n",
    "        for i, (expect, found) in enumerate(test_it):\n",
    "            self.assertEqual(\n",
    "                expect,\n",
    "                found,\n",
    "                f'Index {i} is wrong')\n",
    "        \n",
    "        # Verify both generators are exhausted\n",
    "        try:\n",
    "            next(expect_it)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        else:\n",
    "            self.fail('Expected longer than found')\n",
    "        \n",
    "        try:\n",
    "            next(found_it)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        else:\n",
    "            self.fail('Found longer than expected')\n",
    "    \n",
    "    def test_correct_case(self):\n",
    "        values = [1, 2, 3]\n",
    "        expected = [\n",
    "            1**2,\n",
    "            1**2 + 2**2,\n",
    "            1**2 + 2**2 + 3**2\n",
    "        ]\n",
    "        self.verify_sum_squares(values, expected)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(HelperTestCase)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(suite)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Driven Tests with subTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DataDrivenTestCase(TestCase):\n",
    "    def test_multiple_cases(self):\n",
    "        \"\"\"Test multiple cases without stopping at first failure\"\"\"\n",
    "        test_cases = [\n",
    "            (b'my bytes', 'my bytes'),\n",
    "            ('string', 'string'),\n",
    "            (b'utf-8 bytes', 'utf-8 bytes'),\n",
    "        ]\n",
    "        \n",
    "        for value, expected in test_cases:\n",
    "            with self.subTest(value=value):\n",
    "                self.assertEqual(expected, to_str(value))\n",
    "    \n",
    "    def test_error_cases(self):\n",
    "        \"\"\"Test multiple error conditions\"\"\"\n",
    "        error_cases = [\n",
    "            (object(), TypeError),\n",
    "            (123, TypeError),\n",
    "            (None, TypeError),\n",
    "        ]\n",
    "        \n",
    "        for value, exception in error_cases:\n",
    "            with self.subTest(value=value):\n",
    "                with self.assertRaises(exception):\n",
    "                    to_str(value)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(DataDrivenTestCase)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(suite)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 77: Isolate Tests with setUp, tearDown, setUpModule, and tearDownModule\n",
    "\n",
    "### Test Isolation with setUp and tearDown\n",
    "\n",
    "These methods run before and after each test method, ensuring test isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "class EnvironmentTest(TestCase):\n",
    "    def setUp(self):\n",
    "        \"\"\"Create clean environment before each test\"\"\"\n",
    "        self.test_dir = TemporaryDirectory()\n",
    "        self.test_path = Path(self.test_dir.name)\n",
    "        print(f'\\nSetUp: Created {self.test_path}')\n",
    "    \n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up after each test\"\"\"\n",
    "        print(f'TearDown: Cleaning {self.test_path}')\n",
    "        self.test_dir.cleanup()\n",
    "    \n",
    "    def test_create_file(self):\n",
    "        \"\"\"Test file creation\"\"\"\n",
    "        file_path = self.test_path / 'test.txt'\n",
    "        file_path.write_text('test data')\n",
    "        self.assertTrue(file_path.exists())\n",
    "        print(f'Test: Created {file_path}')\n",
    "    \n",
    "    def test_create_directory(self):\n",
    "        \"\"\"Test directory creation\"\"\"\n",
    "        dir_path = self.test_path / 'subdir'\n",
    "        dir_path.mkdir()\n",
    "        self.assertTrue(dir_path.exists())\n",
    "        print(f'Test: Created {dir_path}')\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(EnvironmentTest)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(suite)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module-Level Setup and Teardown\n",
    "\n",
    "For expensive setup operations, use module-level functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example of module-level setup (would be in a separate module)\n",
    "# This demonstrates the concept\n",
    "\n",
    "# def setUpModule():\n",
    "#     print('* Module setup - expensive operation')\n",
    "#     # Start database, load data, etc.\n",
    "# \n",
    "# def tearDownModule():\n",
    "#     print('* Module teardown - cleanup')\n",
    "#     # Stop database, cleanup resources, etc.\n",
    "\n",
    "class IntegrationTest(TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Alternative: class-level setup\"\"\"\n",
    "        print('\\n* Class setup')\n",
    "    \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        print('* Class teardown')\n",
    "    \n",
    "    def test_one(self):\n",
    "        print('  Test 1')\n",
    "        self.assertTrue(True)\n",
    "    \n",
    "    def test_two(self):\n",
    "        print('  Test 2')\n",
    "        self.assertTrue(True)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(IntegrationTest)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(suite)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 78: Use Mocks to Test Code with Complex Dependencies\n",
    "\n",
    "### Introduction to Mocking\n",
    "\n",
    "Mocks simulate dependencies that are difficult or slow to set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from unittest.mock import Mock\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a mock function\n",
    "def get_animals(database, species):\n",
    "    \"\"\"Would normally query a database\"\"\"\n",
    "    pass\n",
    "\n",
    "# Create a mock\n",
    "mock = Mock(spec=get_animals)\n",
    "expected = [\n",
    "    ('Spot', datetime(2019, 6, 5, 11, 15)),\n",
    "    ('Fluffy', datetime(2019, 6, 5, 12, 30)),\n",
    "    ('Jojo', datetime(2019, 6, 5, 12, 45)),\n",
    "]\n",
    "mock.return_value = expected\n",
    "\n",
    "# Use the mock\n",
    "database = object()  # Sentinel value\n",
    "result = mock(database, 'Meerkat')\n",
    "\n",
    "print('Returned:', result)\n",
    "print('Same as expected:', result == expected)\n",
    "\n",
    "# Verify the mock was called correctly\n",
    "mock.assert_called_once_with(database, 'Meerkat')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ANY for Flexible Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from unittest.mock import ANY\n",
    "\n",
    "mock = Mock(spec=get_animals)\n",
    "mock('database 1', 'Rabbit')\n",
    "mock('database 2', 'Bison')\n",
    "mock('database 3', 'Meerkat')\n",
    "\n",
    "# Verify last call with ANY for database parameter\n",
    "mock.assert_called_with(ANY, 'Meerkat')\n",
    "print('Mock assertions passed!')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mocking Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DatabaseError(Exception):\n",
    "    pass\n",
    "\n",
    "mock = Mock(spec=get_animals)\n",
    "mock.side_effect = DatabaseError('Connection failed')\n",
    "\n",
    "try:\n",
    "    mock(database, 'Meerkat')\n",
    "except DatabaseError as e:\n",
    "    print(f'Caught expected error: {e}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Mock Testing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from datetime import timedelta\n",
    "from unittest.mock import call\n",
    "\n",
    "# Function to test\n",
    "def do_rounds(database, species, *, utcnow=datetime.utcnow):\n",
    "    \"\"\"Feed animals that need feeding\"\"\"\n",
    "    now = utcnow()\n",
    "    feeding_timedelta = database.get_food_period(species)\n",
    "    animals = database.get_animals(species)\n",
    "    fed = 0\n",
    "    \n",
    "    for name, last_mealtime in animals:\n",
    "        if (now - last_mealtime) >= feeding_timedelta:\n",
    "            database.feed_animal(name, now)\n",
    "            fed += 1\n",
    "    \n",
    "    return fed\n",
    "\n",
    "# Mock the database\n",
    "class ZooDatabase:\n",
    "    def get_animals(self, species): pass\n",
    "    def get_food_period(self, species): pass\n",
    "    def feed_animal(self, name, when): pass\n",
    "\n",
    "# Create test\n",
    "database = Mock(spec=ZooDatabase)\n",
    "now_func = Mock(spec=datetime.utcnow)\n",
    "now_func.return_value = datetime(2019, 6, 5, 15, 45)\n",
    "\n",
    "database.get_food_period.return_value = timedelta(hours=3)\n",
    "database.get_animals.return_value = [\n",
    "    ('Spot', datetime(2019, 6, 5, 11, 15)),   # Needs food\n",
    "    ('Fluffy', datetime(2019, 6, 5, 12, 30)), # Needs food\n",
    "    ('Jojo', datetime(2019, 6, 5, 12, 55))    # Recent, doesn't need\n",
    "]\n",
    "\n",
    "# Run the test\n",
    "result = do_rounds(database, 'Meerkat', utcnow=now_func)\n",
    "\n",
    "print(f'Fed {result} animals')\n",
    "\n",
    "# Verify calls\n",
    "database.get_food_period.assert_called_once_with('Meerkat')\n",
    "database.get_animals.assert_called_once_with('Meerkat')\n",
    "database.feed_animal.assert_has_calls(\n",
    "    [\n",
    "        call('Spot', now_func.return_value),\n",
    "        call('Fluffy', now_func.return_value),\n",
    "    ],\n",
    "    any_order=True\n",
    ")\n",
    "\n",
    "print('All assertions passed!')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 79: Encapsulate Dependencies to Facilitate Mocking\n",
    "\n",
    "### Better Design for Testing\n",
    "\n",
    "Encapsulating dependencies in classes makes testing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Bad: Functions as dependencies (harder to mock)\n",
    "# def get_animals(database, species): ...\n",
    "# def get_food_period(database, species): ...\n",
    "# def feed_animal(database, name, when): ...\n",
    "\n",
    "# Good: Class encapsulation (easier to mock)\n",
    "class ZooDatabase:\n",
    "    def __init__(self, connection_string):\n",
    "        self.connection_string = connection_string\n",
    "    \n",
    "    def get_animals(self, species):\n",
    "        \"\"\"Query database for animals\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_food_period(self, species):\n",
    "        \"\"\"Get feeding interval\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def feed_animal(self, name, when):\n",
    "        \"\"\"Record feeding time\"\"\"\n",
    "        pass\n",
    "\n",
    "# Now mocking is straightforward\n",
    "database_mock = Mock(spec=ZooDatabase)\n",
    "print('Created mock:', database_mock)\n",
    "print('Mock method:', database_mock.feed_animal)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Injection Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Global database instance\n",
    "DATABASE = None\n",
    "\n",
    "def get_database():\n",
    "    \"\"\"Get or create database instance\"\"\"\n",
    "    global DATABASE\n",
    "    if DATABASE is None:\n",
    "        DATABASE = ZooDatabase('connection_string')\n",
    "    return DATABASE\n",
    "\n",
    "def main_program(species):\n",
    "    \"\"\"Main program using dependency injection\"\"\"\n",
    "    database = get_database()\n",
    "    count = do_rounds(database, species)\n",
    "    return count\n",
    "\n",
    "# For testing, we can easily inject a mock\n",
    "mock_db = Mock(spec=ZooDatabase)\n",
    "mock_db.get_food_period.return_value = timedelta(hours=3)\n",
    "mock_db.get_animals.return_value = [\n",
    "    ('Test', datetime(2019, 6, 5, 11, 15))\n",
    "]\n",
    "\n",
    "# Inject the mock\n",
    "result = do_rounds(mock_db, 'Meerkat')\n",
    "print(f'Test result: fed {result} animals')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 80: Consider Interactive Debugging with pdb\n",
    "\n",
    "### Using breakpoint()\n",
    "\n",
    "The `breakpoint()` function starts the debugger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import math\n",
    "\n",
    "def compute_rmse(observed, ideal):\n",
    "    \"\"\"Compute Root Mean Square Error\"\"\"\n",
    "    total_err_2 = 0\n",
    "    count = 0\n",
    "    \n",
    "    for got, wanted in zip(observed, ideal):\n",
    "        err_2 = (got - wanted) ** 2\n",
    "        # breakpoint()  # Uncomment to start debugger\n",
    "        total_err_2 += err_2\n",
    "        count += 1\n",
    "    \n",
    "    mean_err = total_err_2 / count\n",
    "    rmse = math.sqrt(mean_err)\n",
    "    return rmse\n",
    "\n",
    "# Test the function\n",
    "result = compute_rmse(\n",
    "    [1.8, 1.7, 3.2, 6],\n",
    "    [2, 1.5, 3, 5]\n",
    ")\n",
    "print(f'RMSE: {result:.2f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_rmse_conditional(observed, ideal):\n",
    "    \"\"\"RMSE with conditional debugging\"\"\"\n",
    "    total_err_2 = 0\n",
    "    count = 0\n",
    "    \n",
    "    for got, wanted in zip(observed, ideal):\n",
    "        err_2 = (got - wanted) ** 2\n",
    "        \n",
    "        # Only debug when error is large\n",
    "        if err_2 >= 1:\n",
    "            # breakpoint()  # Uncomment to debug large errors\n",
    "            print(f'Large error detected: {err_2}')\n",
    "        \n",
    "        total_err_2 += err_2\n",
    "        count += 1\n",
    "    \n",
    "    mean_err = total_err_2 / count\n",
    "    rmse = math.sqrt(mean_err)\n",
    "    return rmse\n",
    "\n",
    "result = compute_rmse_conditional(\n",
    "    [1.8, 1.7, 3.2, 7],\n",
    "    [2, 1.5, 3, 5]\n",
    ")\n",
    "print(f'RMSE: {result:.2f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common pdb Commands\n",
    "\n",
    "When in the debugger:\n",
    "\n",
    "**Inspection:**\n",
    "- `p <expr>` - Print expression\n",
    "- `pp <expr>` - Pretty-print expression\n",
    "- `where` - Show call stack\n",
    "- `up` - Move up call stack\n",
    "- `down` - Move down call stack\n",
    "- `locals()` - Show local variables\n",
    "\n",
    "**Execution:**\n",
    "- `step` / `s` - Step into function\n",
    "- `next` / `n` - Next line (skip function)\n",
    "- `return` / `r` - Return from function\n",
    "- `continue` / `c` - Continue execution\n",
    "- `quit` / `q` - Exit debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 81: Use tracemalloc to Understand Memory Usage\n",
    "\n",
    "### Memory Debugging with gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "class MyObject:\n",
    "    def __init__(self):\n",
    "        self.data = os.urandom(100)\n",
    "\n",
    "def create_objects():\n",
    "    values = []\n",
    "    for _ in range(100):\n",
    "        obj = MyObject()\n",
    "        values.append(obj)\n",
    "    return values\n",
    "\n",
    "# Check object count\n",
    "found_objects = gc.get_objects()\n",
    "print(f'Before: {len(found_objects)} objects')\n",
    "\n",
    "# Create objects\n",
    "hold_reference = create_objects()\n",
    "\n",
    "found_objects = gc.get_objects()\n",
    "print(f'After: {len(found_objects)} objects')\n",
    "print(f'\\nSample objects:')\n",
    "for obj in found_objects[:3]:\n",
    "    print(f'  {repr(obj)[:60]}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Memory Debugging with tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import tracemalloc\n",
    "\n",
    "# Start tracing\n",
    "tracemalloc.start(10)  # Keep 10 stack frames\n",
    "\n",
    "# Take before snapshot\n",
    "time1 = tracemalloc.take_snapshot()\n",
    "\n",
    "# Create objects\n",
    "x = create_objects()\n",
    "\n",
    "# Take after snapshot\n",
    "time2 = tracemalloc.take_snapshot()\n",
    "\n",
    "# Compare snapshots\n",
    "stats = time2.compare_to(time1, 'lineno')\n",
    "\n",
    "print('Top 3 memory allocations:')\n",
    "for stat in stats[:3]:\n",
    "    print(stat)\n",
    "\n",
    "tracemalloc.stop()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Stack Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "tracemalloc.start(10)\n",
    "\n",
    "time1 = tracemalloc.take_snapshot()\n",
    "x = create_objects()\n",
    "time2 = tracemalloc.take_snapshot()\n",
    "\n",
    "# Get statistics by traceback\n",
    "stats = time2.compare_to(time1, 'traceback')\n",
    "top = stats[0]\n",
    "\n",
    "print('Biggest memory offender:')\n",
    "print('\\n'.join(top.traceback.format()))\n",
    "\n",
    "tracemalloc.stop()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Memory Leak Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulate a memory leak\n",
    "leaked_objects = []\n",
    "\n",
    "def potentially_leaky_function():\n",
    "    \"\"\"This function 'leaks' by keeping references\"\"\"\n",
    "    data = [MyObject() for _ in range(50)]\n",
    "    leaked_objects.extend(data)  # Oops, kept reference!\n",
    "    return len(data)\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "snapshots = []\n",
    "for i in range(3):\n",
    "    potentially_leaky_function()\n",
    "    snapshot = tracemalloc.take_snapshot()\n",
    "    snapshots.append(snapshot)\n",
    "\n",
    "# Compare first and last snapshot\n",
    "stats = snapshots[-1].compare_to(snapshots[0], 'lineno')\n",
    "\n",
    "print('Memory growth:')\n",
    "for stat in stats[:3]:\n",
    "    print(stat)\n",
    "\n",
    "tracemalloc.stop()\n",
    "\n",
    "# Clean up\n",
    "leaked_objects.clear()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Testing and Debugging Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Use repr() for debugging** - Makes types and values clear\n",
    "2. **Write comprehensive tests** - Use unittest.TestCase for organized testing\n",
    "3. **Isolate tests** - Use setUp/tearDown for clean test environments\n",
    "4. **Mock dependencies** - Use unittest.mock for complex dependencies\n",
    "5. **Design for testability** - Encapsulate dependencies in classes\n",
    "6. **Interactive debugging** - Use breakpoint() when tests aren't enough\n",
    "7. **Monitor memory** - Use tracemalloc to find memory issues\n",
    "\n",
    "### Testing Hierarchy\n",
    "\n",
    "```\n",
    "Unit Tests (fast, isolated)\n",
    "    ↓\n",
    "Integration Tests (realistic, slower)\n",
    "    ↓\n",
    "End-to-End Tests (complete system)\n",
    "```\n",
    "\n",
    "### When to Use Each Tool\n",
    "\n",
    "- **unittest**: Standard testing framework\n",
    "- **Mock**: Complex dependencies, slow operations\n",
    "- **pdb**: Interactive debugging, unclear failures\n",
    "- **tracemalloc**: Memory leaks, performance issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Write tests for this function\n",
    "def calculate_average(numbers):\n",
    "    \"\"\"Calculate the average of a list of numbers\"\"\"\n",
    "    if not numbers:\n",
    "        raise ValueError('Cannot calculate average of empty list')\n",
    "    return sum(numbers) / len(numbers)\n",
    "\n",
    "# TODO: Write TestCase with multiple test methods\n",
    "# - Test with normal input\n",
    "# - Test with empty list\n",
    "# - Test with single number\n",
    "# - Test with negative numbers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Create a mock for this class\n",
    "class EmailService:\n",
    "    def send_email(self, to, subject, body):\n",
    "        \"\"\"Send an email (would normally contact SMTP server)\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_inbox(self, user):\n",
    "        \"\"\"Get user's inbox (would normally query server)\"\"\"\n",
    "        pass\n",
    "\n",
    "def notify_user(email_service, user, message):\n",
    "    \"\"\"Send notification to user\"\"\"\n",
    "    email_service.send_email(\n",
    "        to=user,\n",
    "        subject='Notification',\n",
    "        body=message\n",
    "    )\n",
    "\n",
    "# TODO: Create a mock for EmailService\n",
    "# TODO: Test notify_user with the mock\n",
    "# TODO: Verify send_email was called correctly"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 3: Debug this function\n",
    "def find_duplicates(items):\n",
    "    \"\"\"Find duplicate items in a list\"\"\"\n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    \n",
    "    for item in items:\n",
    "        if item in seen:\n",
    "            duplicates.append(item)\n",
    "        seen.add(item)\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "# Test it\n",
    "result = find_duplicates([1, 2, 3, 2, 4, 3, 5])\n",
    "print('Duplicates:', result)\n",
    "\n",
    "# TODO: What happens with multiple duplicates?\n",
    "# TODO: Add repr() debugging to understand the issue\n",
    "# TODO: Fix the function if needed"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
