{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d7f8de0",
   "metadata": {},
   "source": [
    "# Chapter 8 — Robustness and Performance (Full conversion)\n",
    "\n",
    "Auto-converted from **Chapter_08_Robustness_and_Performance.pdf**.\n",
    "\n",
    "**Includes:**\n",
    "\n",
    "- Extracted chapter text split by Item headings where possible.\n",
    "- Runnable, commented code cells reproducing examples.\n",
    "- Extra real-world examples for important topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f8a58",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Item 65: Take Advantage of Each Block in try/except /else/finally  301](#Item-65-Take-Advantage-of-Each-Block-in-try-except-else-finally-301)\n",
    "2. [Item 65: Take Advantage of Each Block in try/except /else/finally  303](#Item-65-Take-Advantage-of-Each-Block-in-try-except-else-finally-303)\n",
    "3. [Item 66:  Consider contextlib  and with Statements for ](#Item-66-Consider-contextlib-and-with-Statements-for)\n",
    "4. [Item 54: “Use Lock to Prevent Data Races in Threads”) can be used ](#Item-54-Use-Lock-to-Prevent-Data-Races-in-Threads-can-be-used)\n",
    "5. [Item 66: Consider contextlib  and with Statements 305](#Item-66-Consider-contextlib-and-with-Statements-305)\n",
    "6. [Item ](#Item)\n",
    "7. [Item 26: “Define Function Decorators ](#Item-26-Define-Function-Decorators)\n",
    "8. [Item 30: “Consider Generators Instead of Returning ](#Item-30-Consider-Generators-Instead-of-Returning)\n",
    "9. [Item 35: “Avoid Causing State Transitions in Generators ](#Item-35-Avoid-Causing-State-Transitions-in-Generators)\n",
    "10. [Item 66: Consider contextlib  and with Statements 307](#Item-66-Consider-contextlib-and-with-Statements-307)\n",
    "11. [Item 67: Use datetime  Instead of time for Local Clocks](#Item-67-Use-datetime-Instead-of-time-for-Local-Clocks)\n",
    "12. [Item 67: Use datetime  Instead of time for Local Clocks 309](#Item-67-Use-datetime-Instead-of-time-for-Local-Clocks-309)\n",
    "13. [Item 82: “Know Where to Find Community-Built Modules” for ](#Item-82-Know-Where-to-Find-Community-Built-Modules-for)\n",
    "14. [Item 67: Use datetime  Instead of time for Local Clocks 311](#Item-67-Use-datetime-Instead-of-time-for-Local-Clocks-311)\n",
    "15. [Item 68: Make pickle  Reliable with copyreg](#Item-68-Make-pickle-Reliable-with-copyreg)\n",
    "16. [Item 68: Make pickle  Reliable with copyreg  313](#Item-68-Make-pickle-Reliable-with-copyreg-313)\n",
    "17. [Item 68: Make pickle  Reliable with copyreg  315](#Item-68-Make-pickle-Reliable-with-copyreg-315)\n",
    "18. [Item 68: Make pickle  Reliable with copyreg  317](#Item-68-Make-pickle-Reliable-with-copyreg-317)\n",
    "19. [Item 69: Use decimal  When Precision Is Paramount 319](#Item-69-Use-decimal-When-Precision-Is-Paramount-319)\n",
    "20. [Item 69: Use decimal  When Precision Is Paramount](#Item-69-Use-decimal-When-Precision-Is-Paramount)\n",
    "21. [Item 69: Use decimal  When Precision Is Paramount 321](#Item-69-Use-decimal-When-Precision-Is-Paramount-321)\n",
    "22. [Item 70: Profile Before Optimizing](#Item-70-Profile-Before-Optimizing)\n",
    "23. [Item 70: Profile Before Optimizing 323](#Item-70-Profile-Before-Optimizing-323)\n",
    "24. [Item 72: “Consider Searching Sorted Sequences with bisect ”):](#Item-72-Consider-Searching-Sorted-Sequences-with-bisect)\n",
    "25. [Item 70: Profile Before Optimizing 325](#Item-70-Profile-Before-Optimizing-325)\n",
    "26. [Item 71: Prefer deque  for Producer–Consumer Queues](#Item-71-Prefer-deque-for-Producer-Consumer-Queues)\n",
    "27. [Item 71: Prefer deque  for Producer–Consumer Queues 327](#Item-71-Prefer-deque-for-Producer-Consumer-Queues-327)\n",
    "28. [Item 55: “Use Queue  ](#Item-55-Use-Queue)\n",
    "29. [Item 71: Prefer deque  for Producer–Consumer Queues 329](#Item-71-Prefer-deque-for-Producer-Consumer-Queues-329)\n",
    "30. [Item 71: Prefer deque  for Producer–Consumer Queues 331](#Item-71-Prefer-deque-for-Producer-Consumer-Queues-331)\n",
    "31. [Item 70: “Profile Before Optimizing”).](#Item-70-Profile-Before-Optimizing)\n",
    "32. [Item 71: Prefer deque  for Producer–Consumer Queues 333](#Item-71-Prefer-deque-for-Producer-Consumer-Queues-333)\n",
    "33. [Item 72:  Consider Searching Sorted Sequences ](#Item-72-Consider-Searching-Sorted-Sequences)\n",
    "34. [Item 72: Consider Searching Sorted Sequences with bisect  335](#Item-72-Consider-Searching-Sorted-Sequences-with-bisect-335)\n",
    "35. [Item 43: “Inherit from collections.abc  for Custom Container Types” ](#Item-43-Inherit-from-collections-abc-for-Custom-Container-Types)\n",
    "36. [Item 73: Know How to Use heapq  for Priority Queues](#Item-73-Know-How-to-Use-heapq-for-Priority-Queues)\n",
    "37. [Item 71: “Prefer deque  for Producer–Consumer Queues” and](#Item-71-Prefer-deque-for-Producer-Consumer-Queues-and)\n",
    "38. [Item 55: ](#Item-55)\n",
    "39. [Item 73: Know How to Use heapq  for Priority Queues 337](#Item-73-Know-How-to-Use-heapq-for-Priority-Queues-337)\n",
    "40. [Item 20: “Prefer Raising ](#Item-20-Prefer-Raising)\n",
    "41. [Item ](#Item)\n",
    "42. [Item 73: Know How to Use heapq  for Priority Queues 339](#Item-73-Know-How-to-Use-heapq-for-Priority-Queues-339)\n",
    "43. [Item 14: “Sort by Complex ](#Item-14-Sort-by-Complex)\n",
    "44. [Item 51: “Prefer Class ](#Item-51-Prefer-Class)\n",
    "45. [Item ](#Item)\n",
    "46. [Item 73: Know How to Use heapq  for Priority Queues 341](#Item-73-Know-How-to-Use-heapq-for-Priority-Queues-341)\n",
    "47. [Item 73: Know How to Use heapq  for Priority Queues 343](#Item-73-Know-How-to-Use-heapq-for-Priority-Queues-343)\n",
    "48. [Item 81: “Use tracemalloc  to Understand ](#Item-81-Use-tracemalloc-to-Understand)\n",
    "49. [Item 73: Know How to Use heapq  for Priority Queues 345](#Item-73-Know-How-to-Use-heapq-for-Priority-Queues-345)\n",
    "50. [Item 74:  Consider memoryview  and bytearray  for ](#Item-74-Consider-memoryview-and-bytearray-for)\n",
    "51. [Item 64: “Consider concurrent.futures  for ](#Item-64-Consider-concurrent-futures-for)\n",
    "52. [Item 53: “Use Threads for Blocking I/O, Avoid ](#Item-53-Use-Threads-for-Blocking-I-O-Avoid)\n",
    "53. [Item 60: “Achieve Highly Concurrent I/O with ](#Item-60-Achieve-Highly-Concurrent-I-O-with)\n",
    "54. [Item 74: Consider memoryview  for zero-copy interactions 347](#Item-74-Consider-memoryview-for-zero-copy-interactions-347)\n",
    "55. [Item 61: “Know How to Port Threaded I/O to asyncio ” for what ](#Item-61-Know-How-to-Port-Threaded-I-O-to-asyncio-for-what)\n",
    "56. [Item 11: “Know How to Slice Sequences” for ](#Item-11-Know-How-to-Slice-Sequences-for)\n",
    "57. [Item 74: Consider memoryview  for zero-copy interactions 349](#Item-74-Consider-memoryview-for-zero-copy-interactions-349)\n",
    "58. [Item 74: Consider memoryview  for zero-copy interactions 351](#Item-74-Consider-memoryview-for-zero-copy-interactions-351)\n",
    "59. [Item 90: “Consider Static Analysis via typing  to Obviate ](#Item-90-Consider-Static-Analysis-via-typing-to-Obviate)\n",
    "60. [Item 88: “Know How to Break ](#Item-88-Know-How-to-Break)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097573ee",
   "metadata": {},
   "source": [
    "## Item 65: Take Advantage of Each Block in try/except /else/finally  301\n",
    "\n",
    "Item 65: Take Advantage of Each Block in try/except /else/finally  301\n",
    "readability. For example, say that I want to load JSON dictionary data \n",
    "from a string and return the value of a key it contains:\n",
    "import json\n",
    " \n",
    "def load_json_key(data, key):\n",
    "    try:\n",
    "        print('* Loading JSON data' )\n",
    "        result_dict = json.loads(data)  # May raise ValueError\n",
    "    except ValueError as e:\n",
    "        print('* Handling ValueError' )\n",
    "        raise KeyError(key) from e\n",
    "    else:\n",
    "        print('* Looking up key' )\n",
    "        return result_dict[key]         # May raise KeyError\n",
    "In the successful case, the JSON data is decoded in the try block, \n",
    "and then the key lookup occurs in the else block:\n",
    "assert load_json_key( '{\"foo\": \"bar\"}' , 'foo') == 'bar'\n",
    ">>>\n",
    "* Loading JSON data\n",
    "* Looking up key\n",
    "If the input data isn’t valid JSON, then decoding with json.loads  \n",
    "raises a ValueError . The exception is caught by the except  block and \n",
    "handled:\n",
    "load_json_key( '{\"foo\": bad payload' , 'foo')\n",
    ">>>\n",
    "* Loading JSON data\n",
    "* Handling ValueError\n",
    "Traceback ...\n",
    "JSONDecodeError: Expecting value: line 1 column 9 (char 8)\n",
    " \n",
    "The above exception was the direct cause of the following \n",
    "¯exception:\n",
    " \n",
    "Traceback ...\n",
    "KeyError: 'foo'\n",
    "If the key lookup raises any exceptions, they propagate up to the \n",
    "caller because they are outside the try block. The else clause ensures \n",
    "that what follows the try/except  is visually distinguished from the \n",
    "except  block. This makes the exception propagation behavior clear:\n",
    "load_json_key( '{\"foo\": \"bar\"}' , 'does not exist' )\n",
    "\n",
    "302 Chapter 8 Robustness and Performance\n",
    ">>>\n",
    "* Loading JSON data\n",
    "* Looking up key\n",
    "Traceback ...\n",
    "KeyError: 'does not exist'\n",
    "Everything Together\n",
    "Use try/except /else/finally  when you want to do it all in one com-\n",
    "pound statement. For example, say that I want to read a descrip-\n",
    "tion of work to do from a file, process it, and then update the file \n",
    "in-place. Here, the try block is used to read the file and process it; the \n",
    "except  block is used to handle exceptions from the try block that are \n",
    "expected; the else block is used to update the file in place and allow \n",
    "related exceptions to propagate up; and the finally  block cleans up \n",
    "the file handle:\n",
    "UNDEFINED = object()\n",
    " \n",
    "def divide_json(path):\n",
    "    print('* Opening file' )\n",
    "    handle = open(path, 'r+')   # May raise OSError\n",
    "    try:\n",
    "        print('* Reading data' )\n",
    "        data = handle.read()    # May raise UnicodeDecodeError\n",
    "        print('* Loading JSON data' )\n",
    "        op = json.loads(data)   # May raise ValueError\n",
    "        print('* Performing calculation' )\n",
    "        value = (\n",
    "            op[ 'numerator' ] /\n",
    "            op[ 'denominator' ])  # May raise ZeroDivisionError\n",
    "    except ZeroDivisionError as e:\n",
    "        print('* Handling ZeroDivisionError' )\n",
    "        return UNDEFINED\n",
    "    else:\n",
    "        print('* Writing calculation' )\n",
    "        op[ 'result' ] = value\n",
    "        result = json.dumps(op)\n",
    "        handle.seek( 0)          # May raise OSError\n",
    "        handle.write(result)    # May raise OSError\n",
    "        return value\n",
    "    finally:\n",
    "        print('* Calling close()' )\n",
    "        handle.close()          # Always runs\n",
    "\n",
    "In the successful case, the try, else, and finally  blocks run:\n",
    "temp_path = 'random_data.json'\n",
    " \n",
    "with open(temp_path, 'w') as f:\n",
    "    f.write( '{\"numerator\": 1, \"denominator\": 10}' )\n",
    " \n",
    "assert divide_json(temp_path) == 0.1\n",
    ">>>\n",
    "* Opening file\n",
    "* Reading data\n",
    "* Loading JSON data\n",
    "* Performing calculation\n",
    "* Writing calculation\n",
    "* Calling close()\n",
    "If the calculation is invalid, the try, except , and finally  blocks run, \n",
    "but the else block does not:\n",
    "with open(temp_path, 'w') as f:\n",
    "    f.write( '{\"numerator\": 1, \"denominator\": 0}' )\n",
    " \n",
    "assert divide_json(temp_path) is UNDEFINED\n",
    ">>>\n",
    "* Opening file\n",
    "* Reading data\n",
    "* Loading JSON data\n",
    "* Performing calculation\n",
    "* Handling ZeroDivisionError\n",
    "* Calling close()\n",
    "If the JSON data was invalid, the try block runs and raises an excep-\n",
    "tion, the finally  block runs, and then the exception is propagated up \n",
    "to the caller. The except  and else blocks do not run:\n",
    "with open(temp_path, 'w') as f:\n",
    "    f.write( '{\"numerator\": 1 bad data' )\n",
    " \n",
    "divide_json(temp_path)\n",
    ">>>\n",
    "* Opening file\n",
    "* Reading data\n",
    "* Loading JSON data\n",
    "* Calling close()\n",
    "Traceback ...\n",
    "JSONDecodeError: Expecting ',' delimiter: line 1 column 17 \n",
    "¯(char 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try/except/else/finally example\n",
    "# Example: try/finally usage and the else block pattern\n",
    "import json\n",
    "\n",
    "UNDEFINED = object()\n",
    "\n",
    "def divide_json(path):\n",
    "    print('* Opening file')\n",
    "    handle = open(path, 'r+')   # May raise OSError\n",
    "    try:\n",
    "        print('* Reading data')\n",
    "        data = handle.read()    # May raise UnicodeDecodeError\n",
    "        print('* Loading JSON data')\n",
    "        op = json.loads(data)   # May raise ValueError\n",
    "        print('* Performing calculation')\n",
    "        value = op['numerator'] / op['denominator']  # May raise ZeroDivisionError\n",
    "    except ZeroDivisionError as e:\n",
    "        print('* Handling ZeroDivisionError')\n",
    "        return UNDEFINED\n",
    "    else:\n",
    "        print('* Writing calculation')\n",
    "        op['result'] = value\n",
    "        result = json.dumps(op)\n",
    "        handle.seek(0)\n",
    "        handle.write(result)\n",
    "        handle.truncate()\n",
    "        return value\n",
    "    finally:\n",
    "        print('* Calling close()')\n",
    "        handle.close()\n",
    "\n",
    "# Write a sample file and demonstrate\n",
    "path = '/mnt/data/_sample_divide.json'\n",
    "with open(path, 'w') as f:\n",
    "    f.write('{\"numerator\": 1, \"denominator\": 10}')\n",
    "\n",
    "print('Result:', divide_json(path))\n",
    "with open(path) as f:\n",
    "    print('File after:', f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a648d4",
   "metadata": {},
   "source": [
    "## Item 65: Take Advantage of Each Block in try/except /else/finally  303\n",
    "\n",
    "Item 65: Take Advantage of Each Block in try/except /else/finally  303\n",
    "\n",
    "304 Chapter 8 Robustness and Performance\n",
    "This layout is especially useful because all of the blocks work together \n",
    "in intuitive ways. For example, here I simulate this by running the \n",
    "divide_json  function at the same time that my hard drive runs out of \n",
    "disk space:\n",
    "with open(temp_path, 'w') as f:\n",
    "    f.write( '{\"numerator\": 1, \"denominator\": 10}' )\n",
    " \n",
    "divide_json(temp_path)\n",
    ">>>\n",
    "* Opening file\n",
    "* Reading data\n",
    "* Loading JSON data\n",
    "* Performing calculation\n",
    "* Writing calculation\n",
    "* Calling close()\n",
    "Traceback ...\n",
    "OSError: [Errno 28] No space left on device\n",
    "When the exception was raised in the else block while rewriting the \n",
    "result data, the finally  block still ran and closed the file handle as \n",
    "expected.\n",
    "Things to Remember\n",
    "✦ The try/finally  compound statement lets you run cleanup code \n",
    "regardless of whether exceptions were raised in the try block.\n",
    "✦ The else block helps you minimize the amount of code in try blocks \n",
    "and visually distinguish the success case from the try/except  \n",
    "blocks.\n",
    "✦ An else block can be used to perform additional actions after a suc-\n",
    "cessful try block but before common cleanup in a finally  block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e40d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try/except/else/finally example\n",
    "# Example: try/finally usage and the else block pattern\n",
    "import json\n",
    "\n",
    "UNDEFINED = object()\n",
    "\n",
    "def divide_json(path):\n",
    "    print('* Opening file')\n",
    "    handle = open(path, 'r+')   # May raise OSError\n",
    "    try:\n",
    "        print('* Reading data')\n",
    "        data = handle.read()    # May raise UnicodeDecodeError\n",
    "        print('* Loading JSON data')\n",
    "        op = json.loads(data)   # May raise ValueError\n",
    "        print('* Performing calculation')\n",
    "        value = op['numerator'] / op['denominator']  # May raise ZeroDivisionError\n",
    "    except ZeroDivisionError as e:\n",
    "        print('* Handling ZeroDivisionError')\n",
    "        return UNDEFINED\n",
    "    else:\n",
    "        print('* Writing calculation')\n",
    "        op['result'] = value\n",
    "        result = json.dumps(op)\n",
    "        handle.seek(0)\n",
    "        handle.write(result)\n",
    "        handle.truncate()\n",
    "        return value\n",
    "    finally:\n",
    "        print('* Calling close()')\n",
    "        handle.close()\n",
    "\n",
    "# Write a sample file and demonstrate\n",
    "path = '/mnt/data/_sample_divide.json'\n",
    "with open(path, 'w') as f:\n",
    "    f.write('{\"numerator\": 1, \"denominator\": 10}')\n",
    "\n",
    "print('Result:', divide_json(path))\n",
    "with open(path) as f:\n",
    "    print('File after:', f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0dee7e",
   "metadata": {},
   "source": [
    "## Item 66:  Consider contextlib  and with Statements for \n",
    "\n",
    "Item 66:  Consider contextlib  and with Statements for \n",
    "Reusable try/finally  Behavior\n",
    "The with statement in Python is used to indicate when code is run-\n",
    "ning in a special context. For example, mutual-exclusion locks (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85156ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextmanager example\n",
    "# Example: contextlib.contextmanager to make a debug logging context\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def debug_logging(level):\n",
    "    logger = logging.getLogger()\n",
    "    old_level = logger.getEffectiveLevel()\n",
    "    logger.setLevel(level)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        logger.setLevel(old_level)\n",
    "\n",
    "def my_function():\n",
    "    logging.debug('Some debug data')\n",
    "    logging.error('Error log here')\n",
    "    logging.debug('More debug data')\n",
    "\n",
    "# Demo\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "with debug_logging(logging.DEBUG):\n",
    "    print('* Inside:')\n",
    "    my_function()\n",
    "print('* After:')\n",
    "my_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad1accd",
   "metadata": {},
   "source": [
    "## Item 54: “Use Lock to Prevent Data Races in Threads”) can be used \n",
    "\n",
    "Item 54: “Use Lock to Prevent Data Races in Threads”) can be used \n",
    "in with statements to indicate that the indented code block runs only \n",
    "while the lock is held:\n",
    "from threading import Lock\n",
    " \n",
    "lock = Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba5fea",
   "metadata": {},
   "source": [
    "## Item 66: Consider contextlib  and with Statements 305\n",
    "\n",
    "Item 66: Consider contextlib  and with Statements 305\n",
    "with lock:\n",
    "    # Do something while maintaining an invariant\n",
    "    ...\n",
    "The example above is equivalent to this try/finally  construction \n",
    "because the Lock class properly enables the with statement (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextmanager example\n",
    "# Example: contextlib.contextmanager to make a debug logging context\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def debug_logging(level):\n",
    "    logger = logging.getLogger()\n",
    "    old_level = logger.getEffectiveLevel()\n",
    "    logger.setLevel(level)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        logger.setLevel(old_level)\n",
    "\n",
    "def my_function():\n",
    "    logging.debug('Some debug data')\n",
    "    logging.error('Error log here')\n",
    "    logging.debug('More debug data')\n",
    "\n",
    "# Demo\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "with debug_logging(logging.DEBUG):\n",
    "    print('* Inside:')\n",
    "    my_function()\n",
    "print('* After:')\n",
    "my_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec8e90",
   "metadata": {},
   "source": [
    "## Item \n",
    "\n",
    "Item \n",
    "65: “Take Advantage of Each Block in try/except /else/finally ” for \n",
    "more about try/finally ):\n",
    "lock.acquire()\n",
    "try:\n",
    "    # Do something while maintaining an invariant\n",
    "    ...\n",
    "finally:\n",
    "    lock.release()\n",
    "The with statement version of this is better because it eliminates the \n",
    "need to write the repetitive code of the try/finally  construction, and \n",
    "it ensures that you don’t forget to have a corresponding release  call \n",
    "for every acquire  call.\n",
    "It’s easy to make your objects and functions work in with statements \n",
    "by using the contextlib  b u i l t - i n  m o d u l e .  T h i s  m o d u l e  c o n t a i n s  t h e  \n",
    "contextmanager  decorator (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e24a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try/except/else/finally example\n",
    "# Example: try/finally usage and the else block pattern\n",
    "import json\n",
    "\n",
    "UNDEFINED = object()\n",
    "\n",
    "def divide_json(path):\n",
    "    print('* Opening file')\n",
    "    handle = open(path, 'r+')   # May raise OSError\n",
    "    try:\n",
    "        print('* Reading data')\n",
    "        data = handle.read()    # May raise UnicodeDecodeError\n",
    "        print('* Loading JSON data')\n",
    "        op = json.loads(data)   # May raise ValueError\n",
    "        print('* Performing calculation')\n",
    "        value = op['numerator'] / op['denominator']  # May raise ZeroDivisionError\n",
    "    except ZeroDivisionError as e:\n",
    "        print('* Handling ZeroDivisionError')\n",
    "        return UNDEFINED\n",
    "    else:\n",
    "        print('* Writing calculation')\n",
    "        op['result'] = value\n",
    "        result = json.dumps(op)\n",
    "        handle.seek(0)\n",
    "        handle.write(result)\n",
    "        handle.truncate()\n",
    "        return value\n",
    "    finally:\n",
    "        print('* Calling close()')\n",
    "        handle.close()\n",
    "\n",
    "# Write a sample file and demonstrate\n",
    "path = '/mnt/data/_sample_divide.json'\n",
    "with open(path, 'w') as f:\n",
    "    f.write('{\"numerator\": 1, \"denominator\": 10}')\n",
    "\n",
    "print('Result:', divide_json(path))\n",
    "with open(path) as f:\n",
    "    print('File after:', f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b56db",
   "metadata": {},
   "source": [
    "## Item 26: “Define Function Decorators \n",
    "\n",
    "Item 26: “Define Function Decorators \n",
    "with functools.wraps ” for background), which lets a simple function be \n",
    "used in with statements. This is much easier than defining a new class \n",
    "with the special methods __enter__  and __exit__  (the standard way).\n",
    "For example, say that I want a region of code to have more debug \n",
    "logging sometimes. Here, I define a function that does logging at two \n",
    "severity levels:\n",
    "import logging\n",
    " \n",
    "def my_function():\n",
    "    logging.debug( 'Some debug data' )\n",
    "    logging.error( 'Error log here' )\n",
    "    logging.debug( 'More debug data' )\n",
    "The default log level for my program is WARNING , so only the error mes-\n",
    "sage will print to screen when I run the function:\n",
    "my_function()\n",
    ">>>\n",
    "Error log here\n",
    "\n",
    "306 Chapter 8 Robustness and Performance\n",
    "I can elevate the log level of this function temporarily by defining a \n",
    "context manager. This helper function boosts the logging severity \n",
    "level before running the code in the with block and reduces the log-\n",
    "ging severity level afterward:\n",
    "from contextlib import contextmanager\n",
    " \n",
    "@contextmanager\n",
    "def debug_logging(level):\n",
    "    logger = logging.getLogger()\n",
    "    old_level = logger.getEffectiveLevel()\n",
    "    logger.setLevel(level)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        logger.setLevel(old_level)\n",
    "The yield  expression is the point at which the with block’s contents \n",
    "will execute (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1905809",
   "metadata": {},
   "source": [
    "## Item 30: “Consider Generators Instead of Returning \n",
    "\n",
    "Item 30: “Consider Generators Instead of Returning \n",
    "Lists” for background). Any exceptions that happen in the with block \n",
    "will be re-raised by the yield  expression for you to catch in the helper \n",
    "function (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af9c14",
   "metadata": {},
   "source": [
    "## Item 35: “Avoid Causing State Transitions in Generators \n",
    "\n",
    "Item 35: “Avoid Causing State Transitions in Generators \n",
    "with throw ” for how that works).\n",
    "Now, I can call the same logging function again but in the \n",
    "debug_logging  context. This time, all of the debug messages are \n",
    "printed to the screen during the with block. The same function run-\n",
    "ning outside the with block won’t print debug messages:\n",
    "with debug_logging(logging.DEBUG):\n",
    "    print('* Inside:' )\n",
    "    my_function()\n",
    " \n",
    "print('* After:' )\n",
    "my_function()\n",
    ">>>\n",
    "* Inside:\n",
    "Some debug data\n",
    "Error log here\n",
    "More debug data\n",
    "* After:\n",
    "Error log here\n",
    "Using with Targets\n",
    "The context manager passed to a with statement may also return an \n",
    "object. This object is assigned to a local variable in the as part of the \n",
    "\n",
    "compound statement. This gives the code running in the with block \n",
    "the ability to directly interact with its context. \n",
    "For example, say I want to write a file and ensure that it’s always \n",
    "closed correctly. I can do this by passing open to the with statement. \n",
    "open returns a file handle for the as target of with, and it closes the \n",
    "handle when the with block exits:\n",
    "with open('my_output.txt' , 'w') as handle:\n",
    "    handle.write( 'This is some data!' )\n",
    "This approach is more Pythonic than manually opening and closing \n",
    "the file handle every time. It gives you confidence that the file is even-\n",
    "tually closed when execution leaves the with statement. By highlight-\n",
    "ing the critical section, it also encourages you to reduce the amount \n",
    "of code that executes while the file handle is open, which is good \n",
    "practice in general.\n",
    "To enable your own functions to supply values for as targets, all you \n",
    "need to do is yield  a value from your context manager. For example, \n",
    "here I define a context manager to fetch a Logger  instance, set its \n",
    "level, and then yield  it as the target:\n",
    "@contextmanager\n",
    "def log_level(level, name):\n",
    "    logger = logging.getLogger(name)\n",
    "    old_level = logger.getEffectiveLevel()\n",
    "    logger.setLevel(level)\n",
    "    try:\n",
    "        yield logger\n",
    "    finally:\n",
    "        logger.setLevel(old_level)\n",
    "Calling logging methods like debug  on the as target produces output \n",
    "because the logging severity level is set low enough in the with block \n",
    "on that specific Logger  instance. Using the logging  module directly \n",
    "won’t print anything because the default logging severity level for the \n",
    "default program logger is WARNING :\n",
    "with log_level(logging.DEBUG, 'my-log' ) as logger:\n",
    "    logger.debug( f'This is a message for {logger.name}!' )\n",
    "    logging.debug( 'This will not print' )\n",
    ">>>\n",
    "This is a message for my-log!\n",
    "After the with statement exits, calling debug logging methods on the \n",
    "Logger  named 'my-log'  will not print anything because the default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fc08b",
   "metadata": {},
   "source": [
    "## Item 66: Consider contextlib  and with Statements 307\n",
    "\n",
    "Item 66: Consider contextlib  and with Statements 307\n",
    "\n",
    "308 Chapter 8 Robustness and Performance\n",
    "logging severity level has been restored. Error log messages will \n",
    "always print:\n",
    "logger = logging.getLogger( 'my-log' )\n",
    "logger.debug( 'Debug will not print' )\n",
    "logger.error( 'Error will print' )\n",
    ">>>\n",
    "Error will print\n",
    "Later, I can change the name of the logger I want to use by simply \n",
    "updating the with statement. This will point the Logger  that’s the as \n",
    "target in the with block to a different instance, but I won’t have to \n",
    "update any of my other code to match:\n",
    "with log_level(logging.DEBUG, 'other-log' ) as logger:\n",
    "    logger.debug( f'This is a message for {logger.name}!' )\n",
    "    logging.debug( 'This will not print' )\n",
    ">>>\n",
    "This is a message for other-log!\n",
    "This isolation of state and decoupling between creating a context and \n",
    "acting within that context is another benefit of the with statement.\n",
    "Things to Remember\n",
    "✦ The with statement allows you to reuse logic from try/finally  blocks \n",
    "and reduce visual noise.\n",
    "✦ The contextlib  built-in module provides a contextmanager  decorator \n",
    "that makes it easy to use your own functions in with statements.\n",
    "✦ The value yielded by context managers is supplied to the as part \n",
    "of the with statement. It’s useful for letting your code directly access \n",
    "the cause of a special context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b03c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextmanager example\n",
    "# Example: contextlib.contextmanager to make a debug logging context\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def debug_logging(level):\n",
    "    logger = logging.getLogger()\n",
    "    old_level = logger.getEffectiveLevel()\n",
    "    logger.setLevel(level)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        logger.setLevel(old_level)\n",
    "\n",
    "def my_function():\n",
    "    logging.debug('Some debug data')\n",
    "    logging.error('Error log here')\n",
    "    logging.debug('More debug data')\n",
    "\n",
    "# Demo\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "with debug_logging(logging.DEBUG):\n",
    "    print('* Inside:')\n",
    "    my_function()\n",
    "print('* After:')\n",
    "my_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac05b16",
   "metadata": {},
   "source": [
    "## Item 67: Use datetime  Instead of time for Local Clocks\n",
    "\n",
    "Item 67: Use datetime  Instead of time for Local Clocks\n",
    "Coordinated Universal Time (UTC) is the standard, time-zone- \n",
    "independent representation of time. UTC works great for computers \n",
    "that represent time as seconds since the UNIX epoch. But UTC isn’t \n",
    "ideal for humans. Humans reference time relative to where they’re \n",
    "currently located. People say “noon” or “8 am” instead of “UTC 15:00 \n",
    "minus 7 hours.” If your program handles time, you’ll probably find \n",
    "yourself converting time between UTC and local clocks for the sake of \n",
    "human understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime + pytz example\n",
    "# Example: datetime + pytz conversions (note: pytz may not be installed in this environment)\n",
    "from datetime import datetime, timezone\n",
    "try:\n",
    "    import pytz\n",
    "except Exception:\n",
    "    pytz = None\n",
    "time_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "now = datetime(2019, 3, 16, 22, 14, 35)\n",
    "now_utc = now.replace(tzinfo=timezone.utc)\n",
    "now_local = now_utc.astimezone()\n",
    "print('UTC -> local (system tz):', now_local)\n",
    "\n",
    "if pytz:\n",
    "    arrival_nyc = '2019-03-16 23:33:24'\n",
    "    nyc_dt_naive = datetime.strptime(arrival_nyc, time_format)\n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    nyc_dt = eastern.localize(nyc_dt_naive)\n",
    "    utc_dt = pytz.utc.normalize(nyc_dt.astimezone(pytz.utc))\n",
    "    print('NYC -> UTC:', utc_dt)\n",
    "    pacific = pytz.timezone('US/Pacific')\n",
    "    sf_dt = pacific.normalize(utc_dt.astimezone(pacific))\n",
    "    print('UTC -> SF:', sf_dt)\n",
    "else:\n",
    "    print('pytz not available in this environment; install pytz to run full example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82727be5",
   "metadata": {},
   "source": [
    "## Item 67: Use datetime  Instead of time for Local Clocks 309\n",
    "\n",
    "Item 67: Use datetime  Instead of time for Local Clocks 309\n",
    "Pyth o n  p r o vi d e s  tw o  w a y s  o f  a c c o m p li s hin g  tim e  z o n e  c o n v e r s i o n s .  \n",
    "The old way, using the time built-in module, is terribly error prone. \n",
    "The new way, using the datetime  built-in module, works great with \n",
    "some help from the community-built package named pytz.\n",
    "You should be acquainted with both time and datetime  to thoroughly \n",
    "understand why datetime  i s  t h e  b e s t  c h o i c e  a n d  time should be \n",
    "avoided.\n",
    "The time Module\n",
    "The localtime  function from the time built-in module lets you convert \n",
    "a UNIX timestamp (seconds since the UNIX epoch in UTC) to a local \n",
    "t i m e  t h a t  m a t c h e s  t h e  h o s t  c o m p u t e r ’ s  t i m e  z o n e  ( P a c i f i c  D a y l i g h t  \n",
    "Time in my case). This local time can be printed in human-readable \n",
    "format using the strftime  function:\n",
    "import time\n",
    " \n",
    "now = 1552774475\n",
    "local_tuple = time.localtime(now)\n",
    "time_format = '%Y-%m-%d %H:%M:%S'\n",
    "time_str = time.strftime(time_format, local_tuple)\n",
    "print(time_str)\n",
    ">>>\n",
    "2019-03-16 15:14:35\n",
    "You’ll often need to go the other way as well, starting with user input \n",
    "in human-readable local time and converting it to UTC time. You can \n",
    "do this by using the strptime  function to parse the time string, and \n",
    "then calling mktime  to convert local time to a UNIX timestamp:\n",
    "time_tuple = time.strptime(time_str, time_format)\n",
    "utc_now = time.mktime(time_tuple)\n",
    "print(utc_now)\n",
    ">>>\n",
    "1552774475.0\n",
    "How do you convert local time in one time zone to local time in \n",
    "another time zone? For example, say that I’m taking a flight between \n",
    "San Francisco and New York, and I want to know what time it will be \n",
    "in San Francisco when I’ve arrived in New York.\n",
    "I might initially assume that I can directly manipulate the return val-\n",
    "ues from the time, localtime , and strptime  functions to do time zone \n",
    "conversions. But this is a very bad idea. Time zones change all the time \n",
    "due to local laws. It’s too complicated to manage yourself, especially if \n",
    "you want to handle every global city for flight departures and arrivals.\n",
    "\n",
    "310 Chapter 8 Robustness and Performance\n",
    "Many operating systems have configuration files that keep up with \n",
    "the time zone changes automatically. Python lets you use these time \n",
    "zones through the time module if your platform supports it. On other \n",
    "platforms, such as Windows, some time zone functionality isn’t avail-\n",
    "able from time at all. For example, here I parse a departure time from \n",
    "the San Francisco time zone, Pacific Daylight Time (PDT):\n",
    "import os\n",
    " \n",
    "if os.name == 'nt':\n",
    "    print(\"This example doesn't work on Windows\" )\n",
    "else:\n",
    "    parse_format = '%Y-%m-%d %H:%M:%S %Z'\n",
    "    depart_sfo = '2019-03-16 15:45:16 PDT'\n",
    "    time_tuple = time.strptime(depart_sfo, parse_format)\n",
    "    time_str = time.strftime(time_format, time_tuple)\n",
    "    print(time_str)\n",
    ">>>\n",
    "2019-03-16 15:45:16\n",
    "After seeing that 'PDT'  works with the strptime  function, I might also \n",
    "assume that other time zones known to my computer will work. Unfor-\n",
    "tunately, this isn’t the case. strptime  raises an exception when it sees \n",
    "Eastern Daylight Time (EDT), which is the time zone for New York:\n",
    "arrival_nyc = '2019-03-16 23:33:24 EDT'\n",
    "time_tuple = time.strptime(arrival_nyc, time_format)\n",
    ">>>\n",
    "Traceback ...\n",
    "ValueError: unconverted data remains:  EDT\n",
    "The problem here is the platform-dependent nature of the time mod-\n",
    "ule. Its behavior is determined by how the underlying C functions \n",
    "work with the host operating system. This makes the functionality of \n",
    "the time module unreliable in Python. The time module fails to consis-\n",
    "tently work properly for multiple local times. Thus, you should avoid \n",
    "using the time module for this purpose. If you must use time, use it \n",
    "only to convert between UTC and the host computer’s local time. For \n",
    "all other types of conversions, use the datetime  module.\n",
    "The datetime  Module\n",
    "The second option for representing times in Python is the datetime  \n",
    "class from the datetime  built-in module. Like the time module, \n",
    "datetime  can be used to convert from the current time in UTC to local \n",
    "time.\n",
    "\n",
    "Here, I convert the present time in UTC to my computer’s local time, \n",
    "PDT:\n",
    "from datetime import datetime, timezone\n",
    " \n",
    "now = datetime( 2019, 3, 16, 22, 14, 35)\n",
    "now_utc = now.replace(tzinfo =timezone.utc)\n",
    "now_local = now_utc.astimezone()\n",
    "print(now_local)\n",
    ">>>\n",
    "2019-03-16 15:14:35-07:00\n",
    "The datetime  module can also easily convert a local time back to a \n",
    "UNIX timestamp in UTC:\n",
    "time_str = '2019-03-16 15:14:35'\n",
    "now = datetime.strptime(time_str, time_format)\n",
    "time_tuple = now.timetuple()\n",
    "utc_now = time.mktime(time_tuple)\n",
    "print(utc_now)\n",
    ">>>\n",
    "1552774475.0\n",
    "Unlike the time module, the datetime  module has facilities for reli-\n",
    "ably converting from one local time to another local time. However, \n",
    "datetime  only provides the machinery for time zone operations with \n",
    "its tzinfo  class and related methods. The Python default installation \n",
    "is missing time zone definitions besides UTC.\n",
    "Luckily, the Python community has addressed this gap with the pytz \n",
    "module that’s available for download from the Python Package Index \n",
    "(see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime + pytz example\n",
    "# Example: datetime + pytz conversions (note: pytz may not be installed in this environment)\n",
    "from datetime import datetime, timezone\n",
    "try:\n",
    "    import pytz\n",
    "except Exception:\n",
    "    pytz = None\n",
    "time_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "now = datetime(2019, 3, 16, 22, 14, 35)\n",
    "now_utc = now.replace(tzinfo=timezone.utc)\n",
    "now_local = now_utc.astimezone()\n",
    "print('UTC -> local (system tz):', now_local)\n",
    "\n",
    "if pytz:\n",
    "    arrival_nyc = '2019-03-16 23:33:24'\n",
    "    nyc_dt_naive = datetime.strptime(arrival_nyc, time_format)\n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    nyc_dt = eastern.localize(nyc_dt_naive)\n",
    "    utc_dt = pytz.utc.normalize(nyc_dt.astimezone(pytz.utc))\n",
    "    print('NYC -> UTC:', utc_dt)\n",
    "    pacific = pytz.timezone('US/Pacific')\n",
    "    sf_dt = pacific.normalize(utc_dt.astimezone(pacific))\n",
    "    print('UTC -> SF:', sf_dt)\n",
    "else:\n",
    "    print('pytz not available in this environment; install pytz to run full example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705576ab",
   "metadata": {},
   "source": [
    "## Item 82: “Know Where to Find Community-Built Modules” for \n",
    "\n",
    "Item 82: “Know Where to Find Community-Built Modules” for \n",
    "how to install it). pytz c o n tain s  a  full  d a ta b as e  o f  ev e ry  tim e  zo n e  \n",
    "definition you might need.\n",
    "To use pytz effectively, you should always convert local times to UTC \n",
    "first. Perform any datetime  operations you need on the UTC values \n",
    "(such as offsetting). Then, convert to local times as a final step.\n",
    "For example, here I convert a New York City flight arrival time to a \n",
    "UTC datetime . Although some of these calls seem redundant, all of \n",
    "them are necessary when using pytz:\n",
    "import pytz\n",
    " \n",
    "arrival_nyc = '2019-03-16 23:33:24'\n",
    "nyc_dt_naive = datetime.strptime(arrival_nyc, time_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad25cc",
   "metadata": {},
   "source": [
    "## Item 67: Use datetime  Instead of time for Local Clocks 311\n",
    "\n",
    "Item 67: Use datetime  Instead of time for Local Clocks 311\n",
    "\n",
    "312 Chapter 8 Robustness and Performance\n",
    "eastern = pytz.timezone( 'US/Eastern' )\n",
    "nyc_dt = eastern.localize(nyc_dt_naive)\n",
    "utc_dt = pytz.utc.normalize(nyc_dt.astimezone(pytz.utc))\n",
    "print(utc_dt)\n",
    ">>>\n",
    "2019-03-17 03:33:24+00:00\n",
    "Once I have a UTC datetime , I can convert it to San Francisco local \n",
    "time:\n",
    "pacific = pytz.timezone( 'US/Pacific' )\n",
    "sf_dt = pacific.normalize(utc_dt.astimezone(pacific))\n",
    "print(sf_dt)\n",
    ">>>\n",
    "2019-03-16 20:33:24-07:00\n",
    "Just as easily, I can convert it to the local time in Nepal:\n",
    "nepal = pytz.timezone( 'Asia/Katmandu' )\n",
    "nepal_dt = nepal.normalize(utc_dt.astimezone(nepal))\n",
    "print(nepal_dt)\n",
    ">>>\n",
    "2019-03-17 09:18:24+05:45\n",
    "With datetime  and pytz, these conversions are consistent across all \n",
    "environments, regardless of what operating system the host computer \n",
    "is running.\n",
    "Things to Remember\n",
    "✦ Avoid using the time module for translating between different time \n",
    "zones.\n",
    "✦ Use the datetime  b uil t - in mod ul e al o ng wi th th e pytz community \n",
    "module to reliably convert between times in different time zones.\n",
    "✦ Always represent time in UTC and do conversions to local time as \n",
    "the very final step before presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a65aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime + pytz example\n",
    "# Example: datetime + pytz conversions (note: pytz may not be installed in this environment)\n",
    "from datetime import datetime, timezone\n",
    "try:\n",
    "    import pytz\n",
    "except Exception:\n",
    "    pytz = None\n",
    "time_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "now = datetime(2019, 3, 16, 22, 14, 35)\n",
    "now_utc = now.replace(tzinfo=timezone.utc)\n",
    "now_local = now_utc.astimezone()\n",
    "print('UTC -> local (system tz):', now_local)\n",
    "\n",
    "if pytz:\n",
    "    arrival_nyc = '2019-03-16 23:33:24'\n",
    "    nyc_dt_naive = datetime.strptime(arrival_nyc, time_format)\n",
    "    eastern = pytz.timezone('US/Eastern')\n",
    "    nyc_dt = eastern.localize(nyc_dt_naive)\n",
    "    utc_dt = pytz.utc.normalize(nyc_dt.astimezone(pytz.utc))\n",
    "    print('NYC -> UTC:', utc_dt)\n",
    "    pacific = pytz.timezone('US/Pacific')\n",
    "    sf_dt = pacific.normalize(utc_dt.astimezone(pacific))\n",
    "    print('UTC -> SF:', sf_dt)\n",
    "else:\n",
    "    print('pytz not available in this environment; install pytz to run full example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc233f2",
   "metadata": {},
   "source": [
    "## Item 68: Make pickle  Reliable with copyreg\n",
    "\n",
    "Item 68: Make pickle  Reliable with copyreg\n",
    "The pickle  built-in module can serialize Python objects into a stream \n",
    "of bytes and deserialize bytes back into objects. Pickled byte streams \n",
    "shouldn’t be used to communicate between untrusted parties. The \n",
    "purpose of pickle  is to let you pass Python objects between programs \n",
    "that you control over binary channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9936123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and copyreg example\n",
    "# Example: pickle + copyreg to maintain backward compatibility\n",
    "import pickle, copyreg\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self, level=0, lives=4, points=0):\n",
    "        self.level = level\n",
    "        self.lives = lives\n",
    "        self.points = points\n",
    "\n",
    "def pickle_game_state(game_state):\n",
    "    kwargs = game_state.__dict__.copy()\n",
    "    # add version for future migrations\n",
    "    kwargs['version'] = 1\n",
    "    return unpickle_game_state, (kwargs,)\n",
    "\n",
    "def unpickle_game_state(kwargs):\n",
    "    version = kwargs.pop('version', 1)\n",
    "    # For version 1, nothing to change\n",
    "    return GameState(**kwargs)\n",
    "\n",
    "copyreg.pickle(GameState, pickle_game_state)\n",
    "\n",
    "state = GameState()\n",
    "state.points = 123\n",
    "data = pickle.dumps(state)\n",
    "restored = pickle.loads(data)\n",
    "print('Restored:', restored.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c0a11",
   "metadata": {},
   "source": [
    "## Item 68: Make pickle  Reliable with copyreg  313\n",
    "\n",
    "Item 68: Make pickle  Reliable with copyreg  313\n",
    "Note\n",
    "The pickle  module’s serialization format is unsafe by design. The serialized data \n",
    "contains what is essentially a program that describes how to reconstruct the \n",
    "original Python object. This means a malicious pickle  payload could be used to \n",
    "compromise any part of a Python program that attempts to  deserialize it.\n",
    "In contrast, the json  module is safe by design. Serialized JSON data contains \n",
    "a  s i m p l e  d e s c r i p t i o n  o f  a n  o b j e c t  h i e r a r c h y .  D e s e r i a l i z i n g  J S O N  d a t a  d o e s  \n",
    "not expose a Python program to additional risk. Formats like JSON should \n",
    "be used for communication between programs or people who don’t trust \n",
    "each other.\n",
    "For example, say that I want to use a Python object to represent the \n",
    "state of a player’s progress in a game. The game state includes the \n",
    "level the player is on and the number of lives they have remaining:\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        self.level = 0\n",
    "        self.lives = 4\n",
    "The program modifies this object as the game runs:\n",
    "state = GameState()\n",
    "state.level += 1  # Player beat a level\n",
    "state.lives -= 1  # Player had to try again\n",
    " \n",
    "print(state.__dict__)\n",
    ">>>\n",
    "{'level': 1, 'lives': 3}\n",
    "When the user quits playing, the program can save the state of the \n",
    "game to a file so it can be resumed at a later time. The pickle  mod-\n",
    "ule makes it easy to do this. Here, I use the dump function to write \n",
    "the GameState  object to a file:\n",
    "import pickle\n",
    " \n",
    "state_path = 'game_state.bin'\n",
    "with open(state_path, 'wb') as f:\n",
    "    pickle.dump(state, f)\n",
    "Later, I can call the load function with the file and get back the \n",
    "GameState  object as if it had never been serialized:\n",
    "with open(state_path, 'rb') as f:\n",
    "    state_after = pickle.load(f)\n",
    " \n",
    "print(state_after.__dict__)\n",
    "\n",
    "314 Chapter 8 Robustness and Performance\n",
    ">>>\n",
    "{'level': 1, 'lives': 3}\n",
    "The problem with this approach is what happens as the game’s fea-\n",
    "tures expand over time. Imagine that I want the player to earn points \n",
    "toward a high score. To track the player’s points, I’d add a new field to \n",
    "the GameState  class\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        self.level = 0\n",
    "        self.lives = 4\n",
    "        self.points = 0  # New field\n",
    "Serializing the new version of the GameState  class using pickle  will \n",
    "work exactly as before. Here, I simulate the round-trip through a file \n",
    "by serializing to a string with dumps  and back to an object with loads :\n",
    "state = GameState()\n",
    "serialized = pickle.dumps(state)\n",
    "state_after = pickle.loads(serialized)\n",
    "print(state_after.__dict__)\n",
    ">>>\n",
    "{'level': 0, 'lives': 4, 'points': 0}\n",
    "But what happens to older saved GameState  objects that the user may \n",
    "want to resume? Here, I unpickle an old game file by using a program \n",
    "with the new definition of the GameState  class:\n",
    "with open(state_path, 'rb') as f:\n",
    "    state_after = pickle.load(f)\n",
    " \n",
    "print(state_after.__dict__)\n",
    ">>>\n",
    "{'level': 1, 'lives': 3}\n",
    "The points  attribute is missing! This is especially confusing because \n",
    "the returned object  is an instance of the new GameState  class:\n",
    "assert isinstance (state_after, GameState)\n",
    "This behavior is a byproduct of the way the pickle  module works. Its \n",
    "primary use case is making object serialization easy. As soon as your \n",
    "use of pickle  moves beyond trivial usage, the module’s functionality \n",
    "starts to break down in surprising ways.\n",
    "Fixing these problems is straightforward using the copyreg  built-in \n",
    "module. The copyreg  module lets you register the functions responsible \n",
    "\n",
    "for serializing and deserializing Python objects, allowing you to con-\n",
    "trol the behavior of pickle  and make it more reliable.\n",
    "Default Attribute Values\n",
    "In the simplest case, you can use a constructor with default  argum ents \n",
    "(see I tem 23: “Provide Optional Behavior with Keyword Arguments ” \n",
    "for background) to ensure that GameState  objects will always have all \n",
    "attributes after unpickling. Here, I redefine the constructor this way:\n",
    "class GameState:\n",
    "    def __init__(self, level =0, lives=4, points =0):\n",
    "        self.level = level\n",
    "        self.lives = lives\n",
    "        self.points = points\n",
    "To use this constructor for pickling, I define a helper function that \n",
    "takes a GameState  object and turns it into a tuple  of parameters for \n",
    "the copyreg  module. The returned tuple  contains the function to use \n",
    "for unpickling and the parameters to pass to the unpickling function:\n",
    "def pickle_game_state(game_state):\n",
    "    kwargs = game_state.__dict__\n",
    "    return unpickle_game_state, (kwargs,)\n",
    "N o w ,  I  n e e d  t o  d e f i n e  t h e  unpickle_game_state  helper. This func-\n",
    "tion takes serialized data and parameters from pickle_game_state  \n",
    "and returns the corresponding GameState  object. It’s a tiny wrapper \n",
    "around the constructor:\n",
    "def unpickle_game_state(kwargs):\n",
    "    return GameState( **kwargs)\n",
    "Now, I register these functions with the copyreg  built-in module:\n",
    "import copyreg\n",
    " \n",
    "copyreg.pickle(GameState, pickle_game_state)\n",
    "After registration, serializing and deserializing works as before:\n",
    "state = GameState()\n",
    "state.points += 1000\n",
    "serialized = pickle.dumps(state)\n",
    "state_after = pickle.loads(serialized)\n",
    "print(state_after.__dict__)\n",
    ">>>\n",
    "{'level': 0, 'lives': 4, 'points': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ba8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and copyreg example\n",
    "# Example: pickle + copyreg to maintain backward compatibility\n",
    "import pickle, copyreg\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self, level=0, lives=4, points=0):\n",
    "        self.level = level\n",
    "        self.lives = lives\n",
    "        self.points = points\n",
    "\n",
    "def pickle_game_state(game_state):\n",
    "    kwargs = game_state.__dict__.copy()\n",
    "    # add version for future migrations\n",
    "    kwargs['version'] = 1\n",
    "    return unpickle_game_state, (kwargs,)\n",
    "\n",
    "def unpickle_game_state(kwargs):\n",
    "    version = kwargs.pop('version', 1)\n",
    "    # For version 1, nothing to change\n",
    "    return GameState(**kwargs)\n",
    "\n",
    "copyreg.pickle(GameState, pickle_game_state)\n",
    "\n",
    "state = GameState()\n",
    "state.points = 123\n",
    "data = pickle.dumps(state)\n",
    "restored = pickle.loads(data)\n",
    "print('Restored:', restored.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31449373",
   "metadata": {},
   "source": [
    "## Item 68: Make pickle  Reliable with copyreg  315\n",
    "\n",
    "Item 68: Make pickle  Reliable with copyreg  315\n",
    "\n",
    "316 Chapter 8 Robustness and Performance\n",
    "With this registration done, now I’ll change the definition of GameState  \n",
    "again to give the player a count of magic spells to use. This change is \n",
    "similar to when I added the points  field to GameState :\n",
    "class GameState:\n",
    "    def __init__(self, level =0, lives=4, points =0, magic=5):\n",
    "        self.level = level\n",
    "        self.lives = lives\n",
    "        self.points = points\n",
    "        self.magic = magic  # New field\n",
    "But unlike before, deserializing an old GameState  object will result in \n",
    "valid game data instead of missing attributes. This works because \n",
    "unpickle_game_state  calls the GameState  constructor directly instead \n",
    "of using the pickle  module’s default behavior of saving and restor-\n",
    "ing only the attributes that belong to an object . The GameState  con-\n",
    "structor’s keyword arguments have default values that will be used \n",
    "for any parameters that are missing. This causes old game state \n",
    "files to receive the default value for the new magic  field when they are \n",
    "deserialized:\n",
    "print('Before:' , state.__dict__)\n",
    "state_after = pickle.loads(serialized)\n",
    "print('After: ' , state_after.__dict__)\n",
    ">>>\n",
    "Before: {'level': 0, 'lives': 4, 'points': 1000}\n",
    "After:  {'level': 0, 'lives': 4, 'points': 1000, 'magic': 5}\n",
    "Versioning Classes\n",
    "Sometimes you need to make backward-incompatible changes to your \n",
    "Python objects by removing fields. Doing so prevents the default argu-\n",
    "ment approach above from working.\n",
    "For example, say I realize that a limited number of lives is a bad idea, \n",
    "and I want to remove the concept of lives from the game. Here, I rede-\n",
    "fine the GameState  class to no longer have a lives field:\n",
    "class GameState:\n",
    "    def __init__(self, level =0, points =0, magic=5):\n",
    "        self.level = level\n",
    "        self.points = points\n",
    "        self.magic = magic\n",
    "The problem is that this breaks deserialization of old game data. \n",
    "All fields from the old data, even ones removed from the class, will \n",
    "be passed to th e GameState  constructor by the unpickle_game_state  \n",
    "function:\n",
    "pickle.loads(serialized)\n",
    "\n",
    ">>>\n",
    "Traceback ...\n",
    "TypeError: __init__() got an unexpected keyword argument \n",
    "¯'lives'\n",
    "I can fix this by adding a version parameter to the functions supplied \n",
    "to copyreg . New serialized data will have a version of 2 specified when \n",
    "pickling a new GameState  object:\n",
    "def pickle_game_state(game_state):\n",
    "    kwargs = game_state.__dict__\n",
    "    kwargs[ 'version' ] = 2\n",
    "    return unpickle_game_state, (kwargs,)\n",
    "Old versions of the data will not have a version  argument present, \n",
    "which means I can manipulate the arguments passed to the GameState  \n",
    "constructor accordingly:\n",
    "def unpickle_game_state(kwargs):\n",
    "    version = kwargs.pop( 'version' , 1)\n",
    "    if version == 1:\n",
    "        del kwargs[ 'lives']\n",
    "    return GameState( **kwargs)\n",
    "Now, deserializing an old object works properly:\n",
    "copyreg.pickle(GameState, pickle_game_state)\n",
    "print('Before:' , state.__dict__)\n",
    "state_after = pickle.loads(serialized)\n",
    "print('After: ' , state_after.__dict__)\n",
    ">>>\n",
    "Before: {'level': 0, 'lives': 4, 'points': 1000}\n",
    "After:  {'level': 0, 'points': 1000, 'magic': 5}\n",
    "I can continue using this approach to handle changes between \n",
    "future versions of the same class. Any logic I need to adapt an \n",
    "old version of the class to a new version of the class can go in the \n",
    "unpickle_game_state  function.\n",
    "Stable Import Paths\n",
    "One other issue you may encounter with pickle  i s  b r e a k a g e  f r o m  \n",
    "renaming a class. Often over the life cycle of a program, you’ll refac-\n",
    "tor your code by renaming classes and moving them to other mod-\n",
    "ules. Unfortunately, doing so breaks the pickle  module unless you’re \n",
    "careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ad950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and copyreg example\n",
    "# Example: pickle + copyreg to maintain backward compatibility\n",
    "import pickle, copyreg\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self, level=0, lives=4, points=0):\n",
    "        self.level = level\n",
    "        self.lives = lives\n",
    "        self.points = points\n",
    "\n",
    "def pickle_game_state(game_state):\n",
    "    kwargs = game_state.__dict__.copy()\n",
    "    # add version for future migrations\n",
    "    kwargs['version'] = 1\n",
    "    return unpickle_game_state, (kwargs,)\n",
    "\n",
    "def unpickle_game_state(kwargs):\n",
    "    version = kwargs.pop('version', 1)\n",
    "    # For version 1, nothing to change\n",
    "    return GameState(**kwargs)\n",
    "\n",
    "copyreg.pickle(GameState, pickle_game_state)\n",
    "\n",
    "state = GameState()\n",
    "state.points = 123\n",
    "data = pickle.dumps(state)\n",
    "restored = pickle.loads(data)\n",
    "print('Restored:', restored.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48220602",
   "metadata": {},
   "source": [
    "## Item 68: Make pickle  Reliable with copyreg  317\n",
    "\n",
    "Item 68: Make pickle  Reliable with copyreg  317\n",
    "\n",
    "318 Chapter 8 Robustness and Performance\n",
    "Here, I rename the GameState  class to BetterGameState  and remove \n",
    "the old class from the program entirely:\n",
    "class BetterGameState:\n",
    "    def __init__(self, level =0, points =0, magic=5):\n",
    "        self.level = level\n",
    "        self.points = points\n",
    "        self.magic = magic\n",
    "Attempting to deserialize an old GameState  object now fails because \n",
    "the class can’t be found:\n",
    "pickle.loads(serialized)\n",
    ">>>\n",
    "Traceback ...\n",
    "AttributeError: Can't get attribute 'GameState' on <module \n",
    "¯'__main__' from 'my_code.py'>\n",
    "The cause of this exception is that the import path of the serialized \n",
    "object’s class is encoded in the pickled data:\n",
    "print(serialized)\n",
    ">>>\n",
    "b'\\x80\\x04\\x95A\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\n",
    "¯\\x94\\x8c\\tGameState\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x05level\n",
    "¯\\x94K\\x00\\x8c\\x06points\\x94K\\x00\\x8c\\x05magic\\x94K\\x05ub.'\n",
    "The solution is to use copyreg  again. I can specify a stable identifier \n",
    "for the function to use for unpickling an object. This allows me to \n",
    "transition pickled data to different classes with different names when \n",
    "it’s deserialized. It gives me a level of indirection:\n",
    "copyreg.pickle(BetterGameState, pickle_game_state)\n",
    "After I use copyreg , you can see that the import path to \n",
    "unpickle_game_state  is encoded in the serialized data instead of \n",
    "BetterGameState :\n",
    "state = BetterGameState()\n",
    "serialized = pickle.dumps(state)\n",
    "print(serialized)\n",
    ">>>\n",
    "b'\\x80\\x04\\x95W\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main__\n",
    "¯\\x94\\x8c\\x13unpickle_game_state\\x94\\x93\\x94}\\x94(\\x8c\n",
    "¯\\x05level\\x94K\\x00\\x8c\\x06points\\x94K\\x00\\x8c\\x05magic\\x94K\n",
    "¯\\x05\\x8c\\x07version\\x94K\\x02u\\x85\\x94R\\x94.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle and copyreg example\n",
    "# Example: pickle + copyreg to maintain backward compatibility\n",
    "import pickle, copyreg\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self, level=0, lives=4, points=0):\n",
    "        self.level = level\n",
    "        self.lives = lives\n",
    "        self.points = points\n",
    "\n",
    "def pickle_game_state(game_state):\n",
    "    kwargs = game_state.__dict__.copy()\n",
    "    # add version for future migrations\n",
    "    kwargs['version'] = 1\n",
    "    return unpickle_game_state, (kwargs,)\n",
    "\n",
    "def unpickle_game_state(kwargs):\n",
    "    version = kwargs.pop('version', 1)\n",
    "    # For version 1, nothing to change\n",
    "    return GameState(**kwargs)\n",
    "\n",
    "copyreg.pickle(GameState, pickle_game_state)\n",
    "\n",
    "state = GameState()\n",
    "state.points = 123\n",
    "data = pickle.dumps(state)\n",
    "restored = pickle.loads(data)\n",
    "print('Restored:', restored.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bde75d",
   "metadata": {},
   "source": [
    "## Item 69: Use decimal  When Precision Is Paramount 319\n",
    "\n",
    "Item 69: Use decimal  When Precision Is Paramount 319\n",
    "The only gotcha is that I can’t change the path of the module in \n",
    "which the unpickle_game_state  function is present. Once I serialize \n",
    "data with a function, it must remain available on that import path for \n",
    "deserialization in the future.\n",
    "Things to Remember\n",
    "✦ The pickle  built-in module is useful only for serializing and deseri-\n",
    "alizing objects between trusted programs.\n",
    "✦ Deserializing previously pickled objects may break if the classes \n",
    "involved have changed over time (e.g., attributes have been added \n",
    "or removed).\n",
    "✦ Use the copyreg  b u i l t - i n  m o d u l e  wi th  pickle  t o  e n s u r e  b a c kw ar d  \n",
    "compatibility for serialized objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472fcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decimal precision example\n",
    "# Example: Decimal usage for precise monetary calculations\n",
    "from decimal import Decimal, ROUND_UP\n",
    "\n",
    "rate = Decimal('1.45')\n",
    "seconds = Decimal(3*60 + 42)\n",
    "cost = rate * seconds / Decimal(60)\n",
    "print('Exact cost:', cost)\n",
    "\n",
    "rounded = cost.quantize(Decimal('0.01'), rounding=ROUND_UP)\n",
    "print('Rounded up to cents:', rounded)\n",
    "\n",
    "# Small cheap call rounding\n",
    "small_cost = Decimal('0.004166666666666666666666666667')\n",
    "rounded_small = small_cost.quantize(Decimal('0.01'), rounding=ROUND_UP)\n",
    "print('Small cost rounded up:', rounded_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a002f0a2",
   "metadata": {},
   "source": [
    "## Item 69: Use decimal  When Precision Is Paramount\n",
    "\n",
    "Item 69: Use decimal  When Precision Is Paramount\n",
    "Python is an excellent language for writing code that interacts with \n",
    "numerical data. Python’s integer type can represent values of any \n",
    "practical size. Its double-precision floating point type complies with \n",
    "the IEEE 754 standard. The language also provides a standard com-\n",
    "plex number type for imaginary values. However, these aren’t enough \n",
    "for every situation.\n",
    "For example, say that I want to compute the amount to charge a cus-\n",
    "tomer for an international phone call. I know the time in minutes \n",
    "and seconds that the customer was on the phone (say, 3 minutes \n",
    "42  seconds). I also have a set rate for the cost of calling Antarctica \n",
    "from the United States ($1.45/minute). What should the charge be?\n",
    "With floating point math, the computed charge seems reasonable\n",
    "rate = 1.45\n",
    "seconds = 3*60 + 42\n",
    "cost = rate * seconds / 60\n",
    "print(cost)\n",
    ">>>\n",
    "5.364999999999999\n",
    "The result is 0.0001  short of the correct value ( 5.365 ) due to how IEEE \n",
    "754 floating point numbers are represented. I might want to round up \n",
    "this value to 5.37 to properly cover all costs incurred by the customer. \n",
    "However, due to floating point error, rounding to the nearest whole \n",
    "cent actually reduces the final charge (from 5.364  to 5.36) instead of \n",
    "increasing it (from 5.365  to 5.37):\n",
    "print(round(cost, 2))\n",
    "\n",
    "320 Chapter 8 Robustness and Performance\n",
    ">>>\n",
    "5.36\n",
    "The solution is to use the Decimal  class from the decimal  built-in mod-\n",
    "ule. The Decimal  class provides fixed point math of 28 decimal places \n",
    "by default. It can go even higher, if required. This works around the \n",
    "precision issues in IEEE 754 floating point numbers. The class also \n",
    "gives you more control over rounding behaviors.\n",
    "For example, redoing the Antarctica calculation with Decimal  results \n",
    "in the exact expected charge instead of an approximation:\n",
    "from decimal import Decimal\n",
    " \n",
    "rate = Decimal( '1.45')\n",
    "seconds = Decimal( 3*60 + 42)\n",
    "cost = rate * seconds / Decimal( 60)\n",
    "print(cost)\n",
    ">>>\n",
    "5.365\n",
    "Decimal instances can be given starting values in two different ways. \n",
    "The first way is by passing a str containing the number to the Decimal  \n",
    "constructor. This ensures that there is no loss of precision due to the \n",
    "inh eren t na ture o f Pyth o n fl oa ting po in t n umbers . Th e seco n d wa y \n",
    "is by directly passing a float  or an int instance to the constructor. \n",
    "Here, you can see that the two construction methods result in differ-\n",
    "ent behavior.\n",
    "print(Decimal( '1.45'))\n",
    "print(Decimal( 1.45))\n",
    ">>>\n",
    "1.45\n",
    "1.4499999999999999555910790149937383830547332763671875\n",
    "The same problem doesn’t happen if I supply integers to the Decimal  \n",
    "constructor:\n",
    "print('456')\n",
    "print(456)\n",
    ">>>\n",
    "456\n",
    "456\n",
    "If you care about exact answers, err on the side of caution and use \n",
    "the str constructor for the Decimal  type.\n",
    "\n",
    "Getting back to the phone call example, say that I also want to sup-\n",
    "port very short phone calls between places that are much cheaper \n",
    "to connect (like Toledo and Detroit). Here, I compute the charge for a \n",
    "phone call that was 5 seconds long with a rate of $0.05/minute:\n",
    "rate = Decimal( '0.05')\n",
    "seconds = Decimal( '5')\n",
    "small_cost = rate * seconds / Decimal( 60)\n",
    "print(small_cost)\n",
    ">>>\n",
    "0.004166666666666666666666666667\n",
    "The result is so low that it is decreased to zero when I try to round it \n",
    "to the nearest whole cent. This won’t do!\n",
    "print(round(small_cost, 2))\n",
    ">>>\n",
    "0.00\n",
    "Luckily, the Decimal  class has a built-in function for rounding to \n",
    "exactly the decimal place needed with the desired rounding behavior. \n",
    "This works for the higher cost case from earlier:\n",
    "from decimal import ROUND_UP\n",
    " \n",
    "rounded = cost.quantize(Decimal( '0.01'), rounding =ROUND_UP)\n",
    "print(f'Rounded {cost} to {rounded}' )\n",
    ">>>\n",
    "Rounded 5.365 to 5.37\n",
    "Using the quantize  method this way also properly handles the small \n",
    "usage case for short, cheap phone calls:.\n",
    "rounded = small_cost.quantize(Decimal( '0.01'),\n",
    "                              rounding =ROUND_UP)\n",
    "print(f'Rounded {small_cost} to {rounded}' )\n",
    ">>>\n",
    "Rounded 0.004166666666666666666666666667 to 0.01\n",
    "While Decimal  works great for fixed point numbers, it still has limita-\n",
    "tions in its precision (e.g., 1/3 will be an approximation). For repre-\n",
    "senting rational numbers with no limit to precision, consider using \n",
    "the Fraction  class from the fractions  built-in module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decimal precision example\n",
    "# Example: Decimal usage for precise monetary calculations\n",
    "from decimal import Decimal, ROUND_UP\n",
    "\n",
    "rate = Decimal('1.45')\n",
    "seconds = Decimal(3*60 + 42)\n",
    "cost = rate * seconds / Decimal(60)\n",
    "print('Exact cost:', cost)\n",
    "\n",
    "rounded = cost.quantize(Decimal('0.01'), rounding=ROUND_UP)\n",
    "print('Rounded up to cents:', rounded)\n",
    "\n",
    "# Small cheap call rounding\n",
    "small_cost = Decimal('0.004166666666666666666666666667')\n",
    "rounded_small = small_cost.quantize(Decimal('0.01'), rounding=ROUND_UP)\n",
    "print('Small cost rounded up:', rounded_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37770d",
   "metadata": {},
   "source": [
    "## Item 69: Use decimal  When Precision Is Paramount 321\n",
    "\n",
    "Item 69: Use decimal  When Precision Is Paramount 321\n",
    "\n",
    "322 Chapter 8 Robustness and Performance\n",
    "Things to Remember\n",
    "✦ Python has built-in types and classes in modules that can repre-\n",
    "sent practically every type of numerical value.\n",
    "✦ The Decimal  class is ideal for situations that require high precision \n",
    "and control over rounding behavior, such as computations of mon-\n",
    "etary values.\n",
    "✦ Pass str instances to the Decimal  constructor instead of float  \n",
    "instances if it’s important to compute exact answers and not float-\n",
    "ing point approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af9da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decimal precision example\n",
    "# Example: Decimal usage for precise monetary calculations\n",
    "from decimal import Decimal, ROUND_UP\n",
    "\n",
    "rate = Decimal('1.45')\n",
    "seconds = Decimal(3*60 + 42)\n",
    "cost = rate * seconds / Decimal(60)\n",
    "print('Exact cost:', cost)\n",
    "\n",
    "rounded = cost.quantize(Decimal('0.01'), rounding=ROUND_UP)\n",
    "print('Rounded up to cents:', rounded)\n",
    "\n",
    "# Small cheap call rounding\n",
    "small_cost = Decimal('0.004166666666666666666666666667')\n",
    "rounded_small = small_cost.quantize(Decimal('0.01'), rounding=ROUND_UP)\n",
    "print('Small cost rounded up:', rounded_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2fd09",
   "metadata": {},
   "source": [
    "## Item 70: Profile Before Optimizing\n",
    "\n",
    "Item 70: Profile Before Optimizing\n",
    "The dynamic nature of Python causes surprising behaviors in its run-\n",
    "time performance. Operations you might assume would be slow are \n",
    "a c t u a l l y  v e ry  f a s t  ( e . g . ,  s t r i n g  m a n i p u l a t i o n ,  g e n e r a t o r s ) .  L a n g u a g e  \n",
    "features you might assume would be fast are actually very slow (e.g., \n",
    "attribute accesses, function calls). The true source of slowdowns in a \n",
    "Python program can be obscure.\n",
    "The best approach is to ignore your intuition and directly measure \n",
    "the performance of a program before you try to optimize it. Python \n",
    "provides a built-in profiler  for determining which parts of a program \n",
    "are responsible for its execution time. This means you can focus your \n",
    "optimization efforts on the biggest sources of trouble and ignore parts \n",
    "of the program that don’t impact speed (i.e., follow Amdahl’s law).\n",
    "For example, say that I want to determine why an algorithm in a pro-\n",
    "gram is slow. Here, I define a function that sorts a list of data using \n",
    "an insertion sort:\n",
    "def insertion_sort(data):\n",
    "    result = []\n",
    "    for value in data:\n",
    "        insert_value(result, value)\n",
    "    return result\n",
    "The core mechanism of the insertion sort is the function that finds \n",
    "the insertion point for each piece of data. Here, I define an extremely \n",
    "inefficient version of the insert_value  function that does a linear scan \n",
    "over the input array:\n",
    "def insert_value(array, value):\n",
    "    for i, existing in enumerate (array):\n",
    "        if existing > value:\n",
    "            array.insert(i, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling example\n",
    "# Example: profiling a slow function vs optimized using bisect\n",
    "from random import randint\n",
    "from bisect import bisect_left\n",
    "from cProfile import Profile\n",
    "from pstats import Stats\n",
    "\n",
    "def insertion_sort(data):\n",
    "    result = []\n",
    "    for value in data:\n",
    "        insert_value(result, value)\n",
    "    return result\n",
    "\n",
    "def insert_value(array, value):\n",
    "    for i, existing in enumerate(array):\n",
    "        if existing > value:\n",
    "            array.insert(i, value)\n",
    "            return\n",
    "    array.append(value)\n",
    "\n",
    "def insert_value_bisect(array, value):\n",
    "    i = bisect_left(array, value)\n",
    "    array.insert(i, value)\n",
    "\n",
    "data = [randint(0, 10**4) for _ in range(2000)]\n",
    "p = Profile()\n",
    "p.runcall(lambda: insertion_sort(data))\n",
    "stats = Stats(p)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "print('Profile for insertion_sort (unoptimized):')\n",
    "stats.print_stats(10)\n",
    "\n",
    "p2 = Profile()\n",
    "p2.runcall(lambda: [insert_value_bisect([], v) for v in data])\n",
    "stats2 = Stats(p2)\n",
    "stats2.strip_dirs()\n",
    "stats2.sort_stats('cumulative')\n",
    "print('\\nProfile for bisect-based insert (partial):')\n",
    "stats2.print_stats(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780bee7",
   "metadata": {},
   "source": [
    "## Item 70: Profile Before Optimizing 323\n",
    "\n",
    "Item 70: Profile Before Optimizing 323\n",
    "            return\n",
    "    array.append(value)\n",
    "To profile insertion_sort  and insert_value , I create a data set of ran-\n",
    "dom numbers and define a test function to pass to the profiler:\n",
    "from random import randint\n",
    " \n",
    "max_size = 10**4\n",
    "data = [randint( 0, max_size) for _ in range(max_size)]\n",
    "test = lambda: insertion_sort(data)\n",
    "Python provides two built-in profilers: one that is pure Python \n",
    "(profile ) and another that is a C-extension module ( cProfile ). The \n",
    "cProfile  built -in module is better because of its minimal impact on \n",
    "the performance of your program while it’s being profiled. The pure- \n",
    "Python alternative imposes a high overhead that skews the results.\n",
    "Note\n",
    "When profiling a Python program, be sure that what you’re measuring is the \n",
    "code itself and not external systems. Beware of functions that access the net-\n",
    "work or resources on disk. These may appear to have a large impact on your \n",
    "program’s execution time because of the slowness of the underlying systems. \n",
    "If your program uses a cache to mask the latency of slow resources like these, \n",
    "you should ensure that it’s properly warmed up before you start profiling.\n",
    "Here, I instantiate a Profile  object from the cProfile  module and run \n",
    "the test function through it using the runcall  method:\n",
    "from cProfile import Profile\n",
    " \n",
    "profiler = Profile()\n",
    "profiler.runcall(test)\n",
    "When the test function has finished running, I can extract statistics \n",
    "about its performance by using the pstats  built-in module and its \n",
    "Stats  class. Various methods on a Stats  object adjust how to select \n",
    "and sort the profiling information to show only the things I care \n",
    "about:\n",
    "from pstats import Stats\n",
    " \n",
    "stats = Stats(profiler)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats( 'cumulative' )\n",
    "stats.print_stats()\n",
    "\n",
    "324 Chapter 8 Robustness and Performance\n",
    "The output is a table of information organized by function. The data \n",
    "sample is taken only from the time the profiler was active, during the \n",
    "runcall  method above:\n",
    ">>>\n",
    "         20003 function calls in 1.320 seconds\n",
    " \n",
    "   Ordered by: cumulative time\n",
    " \n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        1    0.000    0.000    1.320    1.320 main.py:35(<lambda>)\n",
    "        1    0.003    0.003    1.320    1.320 main.py:10(insertion_sort)\n",
    "    10000    1.306    0.000    1.317    0.000 main.py:20(insert_value)\n",
    "     9992    0.011    0.000    0.011    0.000 {method 'insert' of 'list' objects}\n",
    "        8    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
    "Here’s a quick guide to what the profiler statistics columns mean:\n",
    " ■ncalls : The number of calls to the function during the profiling \n",
    "period.\n",
    " ■tottime : The number of seconds spent executing the function, \n",
    "excluding time spent executing other functions it calls.\n",
    " ■tottime percall : The average number of seconds spent in the \n",
    "function each time it is called, excluding time spent executing \n",
    "other functions it calls. This is tottime  divided by ncalls .\n",
    " ■cumtime : The cumulative number of seconds spent executing the \n",
    "function, including time spent in all other functions it calls.\n",
    " ■cumtime percall : The average number of seconds spent in the \n",
    "function each time it is called, including time spent in all other \n",
    "functions it calls. This is cumtime  divided by ncalls .\n",
    "Looking at the profiler statistics table above, I can see that the biggest \n",
    "use of CPU in my test is the cumulative time spent in the insert_value  \n",
    "function. Here, I redefine that function to use the bisect  built-in mod-\n",
    "ule (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca28177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling example\n",
    "# Example: profiling a slow function vs optimized using bisect\n",
    "from random import randint\n",
    "from bisect import bisect_left\n",
    "from cProfile import Profile\n",
    "from pstats import Stats\n",
    "\n",
    "def insertion_sort(data):\n",
    "    result = []\n",
    "    for value in data:\n",
    "        insert_value(result, value)\n",
    "    return result\n",
    "\n",
    "def insert_value(array, value):\n",
    "    for i, existing in enumerate(array):\n",
    "        if existing > value:\n",
    "            array.insert(i, value)\n",
    "            return\n",
    "    array.append(value)\n",
    "\n",
    "def insert_value_bisect(array, value):\n",
    "    i = bisect_left(array, value)\n",
    "    array.insert(i, value)\n",
    "\n",
    "data = [randint(0, 10**4) for _ in range(2000)]\n",
    "p = Profile()\n",
    "p.runcall(lambda: insertion_sort(data))\n",
    "stats = Stats(p)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "print('Profile for insertion_sort (unoptimized):')\n",
    "stats.print_stats(10)\n",
    "\n",
    "p2 = Profile()\n",
    "p2.runcall(lambda: [insert_value_bisect([], v) for v in data])\n",
    "stats2 = Stats(p2)\n",
    "stats2.strip_dirs()\n",
    "stats2.sort_stats('cumulative')\n",
    "print('\\nProfile for bisect-based insert (partial):')\n",
    "stats2.print_stats(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231f5ea",
   "metadata": {},
   "source": [
    "## Item 72: “Consider Searching Sorted Sequences with bisect ”):\n",
    "\n",
    "Item 72: “Consider Searching Sorted Sequences with bisect ”):\n",
    "from bisect import bisect_left\n",
    " \n",
    "def insert_value(array, value):\n",
    "    i = bisect_left(array, value)\n",
    "    array.insert(i, value)\n",
    "I can run the profiler again and generate a new table of profiler sta-\n",
    "tistics. The new function is much faster, with a cumulative time spent \n",
    "that is nearly 100 times smaller than with the previous insert_value  \n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bisect example\n",
    "# Example: using bisect for searching in sorted sequences\n",
    "from bisect import bisect_left\n",
    "data = list(range(100000))\n",
    "idx = bisect_left(data, 91234)\n",
    "print('Index via bisect_left:', idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979d506",
   "metadata": {},
   "source": [
    "## Item 70: Profile Before Optimizing 325\n",
    "\n",
    "Item 70: Profile Before Optimizing 325\n",
    ">>>\n",
    "         30003 function calls in 0.017 seconds\n",
    " \n",
    "   Ordered by: cumulative time\n",
    " \n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        1    0.000    0.000    0.017    0.017 main.py:35(<lambda>)\n",
    "        1    0.002    0.002    0.017    0.017 main.py:10(insertion_sort)\n",
    "    10000    0.003    0.000    0.015    0.000 main.py:110(insert_value)\n",
    "    10000    0.008    0.000    0.008    0.000 {method 'insert' of 'list' objects}\n",
    "    10000    0.004    0.000    0.004    0.000 {built-in method _bisect.bisect_left}\n",
    "Sometimes when you’re profiling an entire program, you might find \n",
    "that a common utility function is responsible for the majority of exe-\n",
    "cution time. The default output from the profiler makes such a situ-\n",
    "ation difficult to understand because it doesn’t show that the utility \n",
    "function is called by many different parts of your program.\n",
    "For example, here the my_utility  function is called repeatedly by two \n",
    "different functions in the program:\n",
    "def my_utility(a, b):\n",
    "    c = 1\n",
    "    for i in range(100):\n",
    "        c += a * b\n",
    " \n",
    "def first_func():\n",
    "    for _ in range(1000):\n",
    "        my_utility( 4, 5)\n",
    " \n",
    "def second_func():\n",
    "    for _ in range(10):\n",
    "        my_utility( 1, 3)\n",
    " \n",
    "def my_program():\n",
    "    for _ in range(20):\n",
    "        first_func()\n",
    "        second_func()\n",
    "Profiling this code and using the default print_stats  output gener-\n",
    "ates statistics that are confusing:\n",
    ">>>\n",
    "         20242 function calls in 0.118 seconds\n",
    " \n",
    "   Ordered by: cumulative time\n",
    " \n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        1    0.000    0.000    0.118    0.118 main.py:176(my_program)\n",
    "       20    0.003    0.000    0.117    0.006 main.py:168(first_func)\n",
    "    20200    0.115    0.000    0.115    0.000 main.py:161(my_utility)\n",
    "       20    0.000    0.000    0.001    0.000 main.py:172(second_func)\n",
    "\n",
    "326 Chapter 8 Robustness and Performance\n",
    "The my_utility  function is clearly the source of most execution time, \n",
    "but it’s not immediately obvious why that function is called so much. \n",
    "If you search through the program’s code, you’ll find multiple call \n",
    "sites for my_utility  and still be confused.\n",
    "To deal with this, the Python profiler provides the print_callers  \n",
    "method to show which callers contributed to the profiling information \n",
    "of each function:\n",
    "stats.print_callers()\n",
    "This profiler statistics table shows functions called on the left and \n",
    "which function was responsible for making the call on the right. Here, \n",
    "it’s clear that my_utility  is most used by first_func :\n",
    ">>>\n",
    "   Ordered by: cumulative time\n",
    " \n",
    "Function                                was called by...\n",
    "                                            ncalls  tottime  cumtime\n",
    "main.py:176(my_program)                 <- \n",
    "main.py:168(first_func)                 <-      20    0.003    0.117  main.py:176(my_program)\n",
    "main.py:161(my_utility)                 <-   20000    0.114    0.114  main.py:168(first_func)\n",
    "                                               200    0.001    0.001  main.py:172(second_func)\n",
    "Profiling.md:172(second_func)           <-      20    0.000    0.001  main.py:176(my_program)\n",
    "Things to Remember\n",
    "✦ It’s important to profile Python programs before optimizing because \n",
    "the sources of slowdowns are often obscure.\n",
    "✦ Use the cProfile  module instead of the profile  module because it \n",
    "provides more accurate profiling information.\n",
    "✦ The Profile  object’s runcall  method provides everything you need \n",
    "to profile a tree of function calls in isolation.\n",
    "✦ The Stats  object lets you select and print the subset of profil-\n",
    "ing information you need to see to understand your program’s \n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642333fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling example\n",
    "# Example: profiling a slow function vs optimized using bisect\n",
    "from random import randint\n",
    "from bisect import bisect_left\n",
    "from cProfile import Profile\n",
    "from pstats import Stats\n",
    "\n",
    "def insertion_sort(data):\n",
    "    result = []\n",
    "    for value in data:\n",
    "        insert_value(result, value)\n",
    "    return result\n",
    "\n",
    "def insert_value(array, value):\n",
    "    for i, existing in enumerate(array):\n",
    "        if existing > value:\n",
    "            array.insert(i, value)\n",
    "            return\n",
    "    array.append(value)\n",
    "\n",
    "def insert_value_bisect(array, value):\n",
    "    i = bisect_left(array, value)\n",
    "    array.insert(i, value)\n",
    "\n",
    "data = [randint(0, 10**4) for _ in range(2000)]\n",
    "p = Profile()\n",
    "p.runcall(lambda: insertion_sort(data))\n",
    "stats = Stats(p)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "print('Profile for insertion_sort (unoptimized):')\n",
    "stats.print_stats(10)\n",
    "\n",
    "p2 = Profile()\n",
    "p2.runcall(lambda: [insert_value_bisect([], v) for v in data])\n",
    "stats2 = Stats(p2)\n",
    "stats2.strip_dirs()\n",
    "stats2.sort_stats('cumulative')\n",
    "print('\\nProfile for bisect-based insert (partial):')\n",
    "stats2.print_stats(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde9aa5",
   "metadata": {},
   "source": [
    "## Item 71: Prefer deque  for Producer–Consumer Queues\n",
    "\n",
    "Item 71: Prefer deque  for Producer–Consumer Queues\n",
    "A common need in writing programs is a first-in, first-out (FIFO) \n",
    "queue, which is also known as a producer–consumer queue. A FIFO \n",
    "queue is used when one function gathers values to process and \n",
    "another function handles them in the order in which they were \n",
    "received. Often, programmers use Python’s built-in list t y p e  a s  a  \n",
    "FIFO queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ab0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deque vs list pop(0) microbenchmark\n",
    "# Example: using deque for producer-consumer queues\n",
    "import collections\n",
    "from time import perf_counter\n",
    "\n",
    "def benchmark_pop0(count):\n",
    "    q = list(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.pop(0)\n",
    "    return perf_counter() - t0\n",
    "\n",
    "def benchmark_popleft(count):\n",
    "    q = collections.deque(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.popleft()\n",
    "    return perf_counter() - t0\n",
    "\n",
    "for c in (1000, 2000, 4000):\n",
    "    print(f'pop(0) for {c}:', benchmark_pop0(c))\n",
    "    print(f'popleft for {c}:', benchmark_popleft(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f7b96",
   "metadata": {},
   "source": [
    "## Item 71: Prefer deque  for Producer–Consumer Queues 327\n",
    "\n",
    "Item 71: Prefer deque  for Producer–Consumer Queues 327\n",
    "For example, say that I have a program that’s processing incoming \n",
    "emails for long-term archival, and it’s using a list for a producer–\n",
    "consumer queue. Here, I define a class to represent the messages:\n",
    "class Email:\n",
    "    def __init__(self, sender, receiver, message):\n",
    "        self.sender = sender\n",
    "        self.receiver = receiver\n",
    "        self.message = message\n",
    "    ...\n",
    "I also define a placeholder function for receiving a single email, pre-\n",
    "sumably from a socket, the file system, or some other type of I/O \n",
    "system. The implementation of this function doesn’t matter; what’s \n",
    "important is its interface: It will either return an Email  instance or \n",
    "raise a NoEmailError  exception:\n",
    "class NoEmailError(Exception):\n",
    "    pass\n",
    " \n",
    "def try_receive_email():\n",
    "    # Returns an Email instance or raises NoEmailError\n",
    "    ...\n",
    "The producing function receives emails and enqueues them to be con-\n",
    "sumed at a later time. This function uses the append  method on the \n",
    "list to add new messages to the end of the queue so they are pro-\n",
    "cessed after all messages that were previously received:\n",
    "def produce_emails(queue):\n",
    "    while True:\n",
    "        try:\n",
    "            email = try_receive_email()\n",
    "        except NoEmailError:\n",
    "            return\n",
    "        else:\n",
    "            queue.append(email)  # Producer\n",
    "The consuming function does something useful with the emails. This \n",
    "function calls pop(0)  on the queue, which removes the very first item \n",
    "from the list and returns it to the caller. By always processing items \n",
    "from the beginning of the queue, the consumer ensures that the items \n",
    "are processed in the order in which they were received:\n",
    "def consume_one_email(queue):\n",
    "    if not queue:\n",
    "        return\n",
    "    email = queue.pop( 0)  # Consumer\n",
    "\n",
    "328 Chapter 8 Robustness and Performance\n",
    "    # Index the message for long-term archival\n",
    "    ...\n",
    "Finally, I need a looping function that connects the pieces together. \n",
    "This function alternates between producing and consuming until the \n",
    "keep_running  function returns False  ( s e e  I t e m  6 0 :  “ A c h i e v e  H i g h l y  \n",
    "Concurrent I/O with Coroutines” on how to do this concurrently):\n",
    "def loop(queue, keep_running):\n",
    "    while keep_running():\n",
    "        produce_emails(queue)\n",
    "        consume_one_email(queue)\n",
    " \n",
    "def my_end_func():\n",
    "    ...\n",
    " \n",
    "loop([], my_end_func)\n",
    "Why not process each Email  message in produce_emails  as it’s returned \n",
    "by try_receive_email ? It comes down to the trade-off between latency \n",
    "and throughput. When using producer–consumer queues, you often \n",
    "want to minimize the latency of accepting new items so they can be \n",
    "collected as fast as possible. The consumer can then process through \n",
    "the backlog of items at a consistent pace—one item per loop in this \n",
    "case—which provides a stable performance profile and consistent \n",
    "throughput at the cost of end-to-end latency (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01719fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deque vs list pop(0) microbenchmark\n",
    "# Example: using deque for producer-consumer queues\n",
    "import collections\n",
    "from time import perf_counter\n",
    "\n",
    "def benchmark_pop0(count):\n",
    "    q = list(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.pop(0)\n",
    "    return perf_counter() - t0\n",
    "\n",
    "def benchmark_popleft(count):\n",
    "    q = collections.deque(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.popleft()\n",
    "    return perf_counter() - t0\n",
    "\n",
    "for c in (1000, 2000, 4000):\n",
    "    print(f'pop(0) for {c}:', benchmark_pop0(c))\n",
    "    print(f'popleft for {c}:', benchmark_popleft(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699b4bd",
   "metadata": {},
   "source": [
    "## Item 55: “Use Queue  \n",
    "\n",
    "Item 55: “Use Queue  \n",
    "to Coordinate Work Between Threads” for related best practices).\n",
    "Using a list for a producer–consumer queue like this works fine up \n",
    "to a point, but as the cardinality —the number of items in the list—\n",
    "increases, the list type’s performance can degrade superlinearly. \n",
    "To analyze the performance of using list as a FIFO queue, I can \n",
    "run some micro-benchmarks using the timeit  built-in module. Here, \n",
    "I define a benchmark for the performance of adding new items to the \n",
    "queue using the append  method of list (matching the producer func-\n",
    "tion’s usage):\n",
    "import timeit\n",
    " \n",
    "def print_results(count, tests):\n",
    "    avg_iteration = sum(tests) / len(tests)\n",
    "    print(f'Count {count:>5,} takes {avg_iteration:.6f}s' )\n",
    "    return count, avg_iteration\n",
    " \n",
    "def list_append_benchmark(count):\n",
    "    def run(queue):\n",
    "\n",
    "        for i in range(count):\n",
    "            queue.append(i)\n",
    " \n",
    "    tests = timeit.repeat(\n",
    "        setup ='queue = []' ,\n",
    "        stmt ='run(queue)' ,\n",
    "        globals =locals(),\n",
    "        repeat =1000,\n",
    "        number =1)\n",
    " \n",
    "    return print_results(count, tests)\n",
    "Running this benchmark function with different levels of cardinality \n",
    "lets me compare its performance in relationship to data size:\n",
    "def print_delta(before, after):\n",
    "    before_count, before_time = before\n",
    "    after_count, after_time = after\n",
    "    growth = 1 + (after_count - before_count) / before_count\n",
    "    slowdown = 1 + (after_time - before_time) / before_time\n",
    "    print(f'{growth:>4.1f}x data size, {slowdown:>4.1f}x time' )\n",
    " \n",
    "baseline = list_append_benchmark( 500)\n",
    "for count in (1_000, 2_000, 3_000, 4_000, 5_000):\n",
    "    comparison = list_append_benchmark(count)\n",
    "    print_delta(baseline, comparison)\n",
    ">>>\n",
    "Count   500 takes 0.000039s\n",
    " \n",
    "Count 1,000 takes 0.000073s\n",
    " 2.0x data size,  1.9x time\n",
    " \n",
    "Count 2,000 takes 0.000121s\n",
    " 4.0x data size,  3.1x time\n",
    " \n",
    "Count 3,000 takes 0.000172s\n",
    " 6.0x data size,  4.5x time\n",
    " \n",
    "Count 4,000 takes 0.000240s\n",
    " 8.0x data size,  6.2x time\n",
    " \n",
    "Count 5,000 takes 0.000304s\n",
    "10.0x data size,  7.9x time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19110939",
   "metadata": {},
   "source": [
    "## Item 71: Prefer deque  for Producer–Consumer Queues 329\n",
    "\n",
    "Item 71: Prefer deque  for Producer–Consumer Queues 329\n",
    "\n",
    "330 Chapter 8 Robustness and Performance\n",
    "This shows that the append  method takes roughly constant time for \n",
    "the list type, and the total time for enqueueing scales linearly as the \n",
    "data size increases. There is overhead for the list type to increase its \n",
    "capacity under the covers as new items are added, but it’s reasonably \n",
    "low and is amortized across repeated calls to append .\n",
    "Here, I define a similar benchmark for the pop(0)  call that removes \n",
    "items from the beginning of the queue (matching the consumer func-\n",
    "tion’s usage):\n",
    "def list_pop_benchmark(count):\n",
    "    def prepare():\n",
    "        return list(range(count))\n",
    " \n",
    "    def run(queue):\n",
    "        while queue:\n",
    "            queue.pop( 0)\n",
    " \n",
    "    tests = timeit.repeat(\n",
    "        setup ='queue = prepare()' ,\n",
    "        stmt ='run(queue)' ,\n",
    "        globals =locals(),\n",
    "        repeat =1000,\n",
    "        number =1)\n",
    " \n",
    "    return print_results(count, tests)\n",
    "I can similarly run this benchmark for queues of different sizes to see \n",
    "how performance is affected by cardinality:\n",
    "baseline = list_pop_benchmark( 500)\n",
    "for count in (1_000, 2_000, 3_000, 4_000, 5_000):\n",
    "    comparison = list_pop_benchmark(count)\n",
    "    print_delta(baseline, comparison)\n",
    ">>>\n",
    "Count   500 takes 0.000050s\n",
    " \n",
    "Count 1,000 takes 0.000133s\n",
    " 2.0x data size,  2.7x time\n",
    " \n",
    "Count 2,000 takes 0.000347s\n",
    " 4.0x data size,  6.9x time\n",
    " \n",
    "Count 3,000 takes 0.000663s\n",
    " 6.0x data size, 13.2x time\n",
    " \n",
    "\n",
    "Count 4,000 takes 0.000943s\n",
    " 8.0x data size, 18.8x time\n",
    " \n",
    "Count 5,000 takes 0.001481s\n",
    "10.0x data size, 29.5x time\n",
    "Surprisingly, this shows that the total time for dequeuing items from \n",
    "a list with pop(0)  scales quadratically as the length of the queue \n",
    "increases. The cause is that pop(0)  needs to move every item in the \n",
    "list back an index, effectively reassigning the entire list’s contents. \n",
    "I need to call pop(0)  for every item in the list, and thus I end up \n",
    "doing roughly len(queue)  * len(queue)  operations to consume the \n",
    "queue. This doesn’t scale.\n",
    "Python provides the deque  class from the collections  built-in module \n",
    "to solve this problem. deque  is a double-ended queue  implementation. \n",
    "It provides constant time operations for inserting or removing items \n",
    "from its beginning or end. This makes it ideal for FIFO queues.\n",
    "To use the deque  class, the call to append  in produce_emails  can \n",
    "stay the same as it was when using a list for the queue. The \n",
    "list.pop  method call in consume_one_email  must change to call the \n",
    "deque.popleft  method with no arguments instead. And the loop \n",
    "method must be called with a deque  instance instead of a list. Every-\n",
    "thing else stays the same. Here, I redefine the one function affected to \n",
    "use the new method and run loop again:\n",
    "import collections\n",
    " \n",
    "def consume_one_email(queue):\n",
    "    if not queue:\n",
    "        return\n",
    "    email = queue.popleft()  # Consumer\n",
    "    # Process the email message\n",
    "    ...\n",
    " \n",
    "def my_end_func():\n",
    "    ...\n",
    " \n",
    "loop(collections.deque(), my_end_func)\n",
    "I can run another version of the benchmark to verify that append  \n",
    "performance (matching the producer function’s usage) has stayed \n",
    "roughly the same (modulo a constant factor):\n",
    "def deque_append_benchmark(count):\n",
    "    def prepare():\n",
    "        return collections.deque()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282307b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deque vs list pop(0) microbenchmark\n",
    "# Example: using deque for producer-consumer queues\n",
    "import collections\n",
    "from time import perf_counter\n",
    "\n",
    "def benchmark_pop0(count):\n",
    "    q = list(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.pop(0)\n",
    "    return perf_counter() - t0\n",
    "\n",
    "def benchmark_popleft(count):\n",
    "    q = collections.deque(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.popleft()\n",
    "    return perf_counter() - t0\n",
    "\n",
    "for c in (1000, 2000, 4000):\n",
    "    print(f'pop(0) for {c}:', benchmark_pop0(c))\n",
    "    print(f'popleft for {c}:', benchmark_popleft(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d66ce",
   "metadata": {},
   "source": [
    "## Item 71: Prefer deque  for Producer–Consumer Queues 331\n",
    "\n",
    "Item 71: Prefer deque  for Producer–Consumer Queues 331\n",
    "\n",
    "332 Chapter 8 Robustness and Performance\n",
    "    def run(queue):\n",
    "        for i in range(count):\n",
    "            queue.append(i)\n",
    " \n",
    "    tests = timeit.repeat(\n",
    "        setup ='queue = prepare()' ,\n",
    "        stmt ='run(queue)' ,\n",
    "        globals =locals(),\n",
    "        repeat =1000,\n",
    "        number =1)\n",
    "    return print_results(count, tests)\n",
    " \n",
    "baseline = deque_append_benchmark( 500)\n",
    "for count in (1_000, 2_000, 3_000, 4_000, 5_000):\n",
    "    comparison = deque_append_benchmark(count)\n",
    "    print_delta(baseline, comparison)\n",
    ">>>\n",
    "Count   500 takes 0.000029s\n",
    " \n",
    "Count 1,000 takes 0.000059s\n",
    " 2.0x data size,  2.1x time\n",
    " \n",
    "Count 2,000 takes 0.000121s\n",
    " 4.0x data size,  4.2x time\n",
    " \n",
    "Count 3,000 takes 0.000171s\n",
    " 6.0x data size,  6.0x time\n",
    " \n",
    "Count 4,000 takes 0.000243s\n",
    " 8.0x data size,  8.5x time\n",
    " \n",
    "Count 5,000 takes 0.000295s\n",
    "10.0x data size, 10.3x time\n",
    "And I can benchmark the performance of calling popleft  to mimic \n",
    "the consumer function’s usage of deque :\n",
    "def dequeue_popleft_benchmark(count):\n",
    "    def prepare():\n",
    "        return collections.deque( range(count))\n",
    " \n",
    "    def run(queue):\n",
    "        while queue:\n",
    "            queue.popleft()\n",
    " \n",
    "    tests = timeit.repeat(\n",
    "\n",
    "        setup ='queue = prepare()' ,\n",
    "        stmt ='run(queue)' ,\n",
    "        globals =locals(),\n",
    "        repeat =1000,\n",
    "        number =1)\n",
    " \n",
    "    return print_results(count, tests)\n",
    " \n",
    "baseline = dequeue_popleft_benchmark( 500)\n",
    "for count in (1_000, 2_000, 3_000, 4_000, 5_000):\n",
    "    comparison = dequeue_popleft_benchmark(count)\n",
    "    print_delta(baseline, comparison)\n",
    ">>>\n",
    "Count   500 takes 0.000024s\n",
    " \n",
    "Count 1,000 takes 0.000050s\n",
    " 2.0x data size,  2.1x time\n",
    " \n",
    "Count 2,000 takes 0.000100s\n",
    " 4.0x data size,  4.2x time\n",
    " \n",
    "Count 3,000 takes 0.000152s\n",
    " 6.0x data size,  6.3x time\n",
    " \n",
    "Count 4,000 takes 0.000207s\n",
    " 8.0x data size,  8.6x time\n",
    " \n",
    "Count 5,000 takes 0.000265s\n",
    "10.0x data size, 11.0x time\n",
    "The popleft  usage scales linearly instead of displaying the super-\n",
    "linear behavior of pop(0)  th a t  I  m e a s u r e d  b e f o r e— h o o r a y !  I f  y o u  \n",
    "know that the performance of a program critically depends on the \n",
    "speed of producer–consumer queues, then deque  is a great choice. \n",
    "If you’re not sure, then you should instrument your program to \n",
    "find out (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deque vs list pop(0) microbenchmark\n",
    "# Example: using deque for producer-consumer queues\n",
    "import collections\n",
    "from time import perf_counter\n",
    "\n",
    "def benchmark_pop0(count):\n",
    "    q = list(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.pop(0)\n",
    "    return perf_counter() - t0\n",
    "\n",
    "def benchmark_popleft(count):\n",
    "    q = collections.deque(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.popleft()\n",
    "    return perf_counter() - t0\n",
    "\n",
    "for c in (1000, 2000, 4000):\n",
    "    print(f'pop(0) for {c}:', benchmark_pop0(c))\n",
    "    print(f'popleft for {c}:', benchmark_popleft(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a01645",
   "metadata": {},
   "source": [
    "## Item 70: “Profile Before Optimizing”).\n",
    "\n",
    "Item 70: “Profile Before Optimizing”).\n",
    "Things to Remember\n",
    "✦ The list type can be used as a FIFO queue by having the producer \n",
    "call append  to add items and the consumer call pop(0)  to receive \n",
    "items. However, this may cause problems because the performance \n",
    "of pop(0)  degrades superlinearly as the queue length increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling example\n",
    "# Example: profiling a slow function vs optimized using bisect\n",
    "from random import randint\n",
    "from bisect import bisect_left\n",
    "from cProfile import Profile\n",
    "from pstats import Stats\n",
    "\n",
    "def insertion_sort(data):\n",
    "    result = []\n",
    "    for value in data:\n",
    "        insert_value(result, value)\n",
    "    return result\n",
    "\n",
    "def insert_value(array, value):\n",
    "    for i, existing in enumerate(array):\n",
    "        if existing > value:\n",
    "            array.insert(i, value)\n",
    "            return\n",
    "    array.append(value)\n",
    "\n",
    "def insert_value_bisect(array, value):\n",
    "    i = bisect_left(array, value)\n",
    "    array.insert(i, value)\n",
    "\n",
    "data = [randint(0, 10**4) for _ in range(2000)]\n",
    "p = Profile()\n",
    "p.runcall(lambda: insertion_sort(data))\n",
    "stats = Stats(p)\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('cumulative')\n",
    "print('Profile for insertion_sort (unoptimized):')\n",
    "stats.print_stats(10)\n",
    "\n",
    "p2 = Profile()\n",
    "p2.runcall(lambda: [insert_value_bisect([], v) for v in data])\n",
    "stats2 = Stats(p2)\n",
    "stats2.strip_dirs()\n",
    "stats2.sort_stats('cumulative')\n",
    "print('\\nProfile for bisect-based insert (partial):')\n",
    "stats2.print_stats(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dea67c",
   "metadata": {},
   "source": [
    "## Item 71: Prefer deque  for Producer–Consumer Queues 333\n",
    "\n",
    "Item 71: Prefer deque  for Producer–Consumer Queues 333\n",
    "\n",
    "334 Chapter 8 Robustness and Performance\n",
    "✦ The deque  class from the collections  built-in module takes constant \n",
    "time—regardless of length—for append  and popleft , making it ideal \n",
    "for FIFO queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deque vs list pop(0) microbenchmark\n",
    "# Example: using deque for producer-consumer queues\n",
    "import collections\n",
    "from time import perf_counter\n",
    "\n",
    "def benchmark_pop0(count):\n",
    "    q = list(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.pop(0)\n",
    "    return perf_counter() - t0\n",
    "\n",
    "def benchmark_popleft(count):\n",
    "    q = collections.deque(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.popleft()\n",
    "    return perf_counter() - t0\n",
    "\n",
    "for c in (1000, 2000, 4000):\n",
    "    print(f'pop(0) for {c}:', benchmark_pop0(c))\n",
    "    print(f'popleft for {c}:', benchmark_popleft(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2b389",
   "metadata": {},
   "source": [
    "## Item 72:  Consider Searching Sorted Sequences \n",
    "\n",
    "Item 72:  Consider Searching Sorted Sequences \n",
    "with bisect\n",
    "It’s common to find yourself with a large amount of data in memory \n",
    "as a sorted list that you then want to search. For example, you may \n",
    "ha v e l oa d ed an Englis h languag e di c ti o nary to u se f o r s pe ll ch eck -\n",
    "ing, or perhaps a list of dated financial transactions to audit for \n",
    "correctness.\n",
    "Regardless of the data your specific program needs to process, search-\n",
    "ing for a specific value in a list takes linear time proportional to the \n",
    "list’s length when you call the index  method:\n",
    "data = list(range(10**5))\n",
    "index = data.index( 91234)\n",
    "assert index == 91234\n",
    "If you’re not sure whether the exact value you’re searching for is in the \n",
    "list, then you may want to search for the closest index that is equal \n",
    "to or exceeds your goal value. The simplest way to do this is to lin-\n",
    "early scan the list and compare each item to your goal value:\n",
    "def find_closest(sequence, goal):\n",
    "    for index, value in enumerate (sequence):\n",
    "        if goal < value:\n",
    "            return index\n",
    "    raise ValueError( f'{goal} is out of bounds' )\n",
    " \n",
    "index = find_closest(data, 91234.56 )\n",
    "assert index == 91235\n",
    "Python’s built-in bisect  m o d u l e  p r o v i d e s  b e t t e r  w a y s  t o  a c c o m -\n",
    "plish these types of searches through ordered lists. You can use the \n",
    "bisect_left  f u n c t i o n  t o  d o  a n  e f f i c i e n t  b i n a ry  s e a r c h  t h r o u g h  a n y  \n",
    "sequence of sorted items. The index it returns will either be where the \n",
    "item is already present in the list or where you’d want to insert the \n",
    "item in the list to keep it in sorted order:\n",
    "from bisect import bisect_left\n",
    " \n",
    "index = bisect_left(data, 91234)     # Exact match\n",
    "assert index == 91234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fd956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bisect example\n",
    "# Example: using bisect for searching in sorted sequences\n",
    "from bisect import bisect_left\n",
    "data = list(range(100000))\n",
    "idx = bisect_left(data, 91234)\n",
    "print('Index via bisect_left:', idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ca4c0",
   "metadata": {},
   "source": [
    "## Item 72: Consider Searching Sorted Sequences with bisect  335\n",
    "\n",
    "Item 72: Consider Searching Sorted Sequences with bisect  335\n",
    "index = bisect_left(data, 91234.56 )  # Closest match\n",
    "assert index == 91235\n",
    "The complexity of the binary search algorithm used by the bisect  \n",
    "module is logarithmic. This means searching in a list of length \n",
    "1 million takes roughly the same amount of time with bisect  as \n",
    "linearly searching a list o f  l e ngth  20  u s ing  th e  list.index  method \n",
    "(math.log2(10**6) == 19.93... ). It’s way faster!\n",
    "I can verify this speed improvement for the example from above by \n",
    "using the timeit  built-in module to run a micro-benchmark:\n",
    "import random\n",
    "import timeit\n",
    " \n",
    "size = 10**5\n",
    "iterations = 1000\n",
    " \n",
    "data = list(range(size))\n",
    "to_lookup = [random.randint( 0, size)\n",
    "             for _ in range(iterations)]\n",
    " \n",
    "def run_linear(data, to_lookup):\n",
    "    for index in to_lookup:\n",
    "        data.index(index)\n",
    " \n",
    "def run_bisect(data, to_lookup):\n",
    "    for index in to_lookup:\n",
    "        bisect_left(data, index)\n",
    " \n",
    "baseline = timeit.timeit(\n",
    "    stmt ='run_linear(data, to_lookup)' ,\n",
    "    globals =globals(),\n",
    "    number =10)\n",
    "print(f'Linear search takes {baseline:.6f}s' )\n",
    " \n",
    "comparison = timeit.timeit(\n",
    "    stmt ='run_bisect(data, to_lookup)' ,\n",
    "    globals =globals(),\n",
    "    number =10)\n",
    "print(f'Bisect search takes {comparison:.6f}s' )\n",
    " \n",
    "slowdown = 1 + ((baseline - comparison) / comparison)\n",
    "print(f'{slowdown:.1f}x time' )\n",
    "\n",
    "336 Chapter 8 Robustness and Performance\n",
    ">>>\n",
    "Linear search takes 5.370117s\n",
    "Bisect search takes 0.005220s\n",
    "1028.7x time\n",
    "The best part about bisect  is that it’s not limited to the list type; \n",
    "you can use it with any Python object that acts like a sequence (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82482fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bisect example\n",
    "# Example: using bisect for searching in sorted sequences\n",
    "from bisect import bisect_left\n",
    "data = list(range(100000))\n",
    "idx = bisect_left(data, 91234)\n",
    "print('Index via bisect_left:', idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac01294",
   "metadata": {},
   "source": [
    "## Item 43: “Inherit from collections.abc  for Custom Container Types” \n",
    "\n",
    "Item 43: “Inherit from collections.abc  for Custom Container Types” \n",
    "for how to do that). The module also provides additional features for \n",
    "more advanced situations (see help(bisect) ).\n",
    "Things to Remember\n",
    "✦ Searching sorted data contained in a list takes linear time using \n",
    "the index  method or a for loop with simple comparisons.\n",
    "✦ The bisect  built-in module’s bisect_left  function takes logarith-\n",
    "mic time to search for values in sorted lists, which can be orders of \n",
    "magnitude faster than other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6eb67",
   "metadata": {},
   "source": [
    "## Item 73: Know How to Use heapq  for Priority Queues\n",
    "\n",
    "Item 73: Know How to Use heapq  for Priority Queues\n",
    "One of the limitations of Python’s other queue implementations (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heapq priority queue example\n",
    "# Example: using heapq for priority queue\n",
    "import heapq\n",
    "class Book:\n",
    "    def __init__(self, title, due_date):\n",
    "        self.title = title\n",
    "        self.due_date = due_date\n",
    "    def __lt__(self, other):\n",
    "        # heapq is a min-heap; earlier due_date is higher priority\n",
    "        return self.due_date < other.due_date\n",
    "heap = []\n",
    "heapq.heappush(heap, Book('Don Quixote', '2019-06-07'))\n",
    "heapq.heappush(heap, Book('Frankenstein', '2019-06-05'))\n",
    "heapq.heappush(heap, Book('War and Peace', '2019-06-03'))\n",
    "while heap:\n",
    "    b = heapq.heappop(heap)\n",
    "    print('Next due:', b.title, b.due_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b6169",
   "metadata": {},
   "source": [
    "## Item 71: “Prefer deque  for Producer–Consumer Queues” and\n",
    "\n",
    "Item 71: “Prefer deque  for Producer–Consumer Queues” and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deque vs list pop(0) microbenchmark\n",
    "# Example: using deque for producer-consumer queues\n",
    "import collections\n",
    "from time import perf_counter\n",
    "\n",
    "def benchmark_pop0(count):\n",
    "    q = list(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.pop(0)\n",
    "    return perf_counter() - t0\n",
    "\n",
    "def benchmark_popleft(count):\n",
    "    q = collections.deque(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.popleft()\n",
    "    return perf_counter() - t0\n",
    "\n",
    "for c in (1000, 2000, 4000):\n",
    "    print(f'pop(0) for {c}:', benchmark_pop0(c))\n",
    "    print(f'popleft for {c}:', benchmark_popleft(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d5fc6c",
   "metadata": {},
   "source": [
    "## Item 55: \n",
    "\n",
    "Item 55: \n",
    "“Use Queue  t o  C o o r din a t e  W o r k  B e tw e e n  Thr e a d s ”)  i s  th a t  th e y  ar e  \n",
    "first-in, first-out (FIFO) queues: Their contents are sorted by the order \n",
    "in which they were received. Often, you need a program to process \n",
    "items in order of relative importance instead. To accomplish this, a \n",
    "priority queue  is the right tool for the job.\n",
    "For example, say that I’m writing a program to manage books bor-\n",
    "rowed from a library. There are people constantly borrowing new \n",
    "books. There are people returning their borrowed books on time. And \n",
    "there are people who need to be reminded to return their overdue \n",
    "books. Here, I define a class to represent a book that’s been borrowed:\n",
    "class Book:\n",
    "    def __init__(self, title, due_date):\n",
    "        self.title = title\n",
    "        self.due_date = due_date\n",
    "I need a system that will send reminder messages when each book \n",
    "passes its due date. Unfortunately, I can’t use a FIFO queue for this \n",
    "because the amount of time each book is allowed to be borrowed var-\n",
    "ies based on its recency, popularity, and other factors. For example, a \n",
    "book that is borrowed today may be due back later than a book that’s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581597b9",
   "metadata": {},
   "source": [
    "## Item 73: Know How to Use heapq  for Priority Queues 337\n",
    "\n",
    "Item 73: Know How to Use heapq  for Priority Queues 337\n",
    "borrowed tomorrow. Here, I achieve this behavior by using a standard \n",
    "list and sorting it by due_date  each time a new Book is added:\n",
    "def add_book(queue, book):\n",
    "    queue.append(book)\n",
    "    queue.sort(key =lambda x: x.due_date, reverse =True)\n",
    " \n",
    "queue = []\n",
    "add_book(queue, Book( 'Don Quixote' , '2019-06-07' ))\n",
    "add_book(queue, Book( 'Frankenstein' , '2019-06-05' ))\n",
    "add_book(queue, Book( 'Les Misérables' , '2019-06-08' ))\n",
    "add_book(queue, Book( 'War and Peace' , '2019-06-03' ))\n",
    "If I can assume that the queue of borrowed books is always in sorted \n",
    "order, then all I need to do to check for overdue books is to inspect the \n",
    "final element in the list. Here, I define a function to return the next \n",
    "overdue book, if any, and remove it from the queue:\n",
    "class NoOverdueBooks(Exception):\n",
    "    pass\n",
    " \n",
    "def next_overdue_book(queue, now):\n",
    "    if queue:\n",
    "        book = queue[-1]\n",
    "        if book.due_date < now:\n",
    "            queue.pop()\n",
    "            return book\n",
    " \n",
    "    raise NoOverdueBooks\n",
    "I can call this function repeatedly to get overdue books to remind peo-\n",
    "ple about in the order of most overdue to least overdue:\n",
    "now = '2019-06-10'\n",
    " \n",
    "found = next_overdue_book(queue, now)\n",
    "print(found.title)\n",
    " \n",
    "found = next_overdue_book(queue, now)\n",
    "print(found.title)\n",
    ">>>\n",
    "War and Peace\n",
    "Frankenstein\n",
    "\n",
    "338 Chapter 8 Robustness and Performance\n",
    "If a book is returned before the due date, I can remove the scheduled \n",
    "reminder message by removing the Book from the list:\n",
    "def return_book(queue, book):\n",
    "    queue.remove(book)\n",
    " \n",
    "queue = []\n",
    "book = Book('Treasure Island' , '2019-06-04' )\n",
    " \n",
    "add_book(queue, book)\n",
    "print('Before return:' , [x.title for x in queue])\n",
    " \n",
    "return_book(queue, book)\n",
    "print('After return: ' , [x.title for x in queue])\n",
    ">>>\n",
    "Before return: ['Treasure Island']\n",
    "After return:  []\n",
    "And I can confirm that when all books are returned, the return_book  \n",
    "function will raise the right exception (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee30564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heapq priority queue example\n",
    "# Example: using heapq for priority queue\n",
    "import heapq\n",
    "class Book:\n",
    "    def __init__(self, title, due_date):\n",
    "        self.title = title\n",
    "        self.due_date = due_date\n",
    "    def __lt__(self, other):\n",
    "        # heapq is a min-heap; earlier due_date is higher priority\n",
    "        return self.due_date < other.due_date\n",
    "heap = []\n",
    "heapq.heappush(heap, Book('Don Quixote', '2019-06-07'))\n",
    "heapq.heappush(heap, Book('Frankenstein', '2019-06-05'))\n",
    "heapq.heappush(heap, Book('War and Peace', '2019-06-03'))\n",
    "while heap:\n",
    "    b = heapq.heappop(heap)\n",
    "    print('Next due:', b.title, b.due_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa556ba",
   "metadata": {},
   "source": [
    "## Item 20: “Prefer Raising \n",
    "\n",
    "Item 20: “Prefer Raising \n",
    "Exceptions to Returning None”):\n",
    "try:\n",
    "    next_overdue_book(queue, now)\n",
    "except NoOverdueBooks:\n",
    "    pass          # Expected\n",
    "else:\n",
    "    assert False  # Doesn't happen\n",
    "However, the computational complexity of this solution isn’t \n",
    "i d e a l .  A l t h o u g h  c h e c k i n g  f o r  a n d  r e m o v i n g  a n  o v e r d u e  b o o k  h a s  \n",
    "a constant cost, every time I add a book, I pay the cost of sorting \n",
    "the whole list a g a i n .  I f  I  h a v e  len(queue)  books to add, and the \n",
    "c o s t  o f  s o r t i n g  t h e m  i s  r o u g h l y  len(queue)  * math.log(len(queue)) , \n",
    "the time it takes to add books will grow superlinearly \n",
    "(len(queue) * len(queue) * math.log(len(queue)) ).\n",
    "Here, I define a micro-benchmark to measure this performance \n",
    "behavior experimentally by using the timeit  built-in module (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000053bd",
   "metadata": {},
   "source": [
    "## Item \n",
    "\n",
    "Item \n",
    "71: “Prefer deque  for Producer–Consumer Queues” for the implemen-\n",
    "tation of print_results  and print_delta ):\n",
    "import random\n",
    "import timeit\n",
    " \n",
    "def print_results(count, tests):\n",
    "    ...\n",
    " \n",
    "\n",
    "def print_delta(before, after):\n",
    "    ...\n",
    " \n",
    "def list_overdue_benchmark(count):\n",
    "    def prepare():\n",
    "        to_add = list(range(count))\n",
    "        random.shuffle(to_add)\n",
    "        return [], to_add\n",
    " \n",
    "    def run(queue, to_add):\n",
    "        for i in to_add:\n",
    "            queue.append(i)\n",
    "            queue.sort(reverse =True)\n",
    " \n",
    "        while queue:\n",
    "            queue.pop()\n",
    " \n",
    "    tests = timeit.repeat(\n",
    "        setup ='queue, to_add = prepare()' ,\n",
    "        stmt =f'run(queue, to_add)' ,\n",
    "        globals =locals(),\n",
    "        repeat =100,\n",
    "        number =1)\n",
    " \n",
    "    return print_results(count, tests)\n",
    "I can verify that the runtime of adding and removing books from the \n",
    "queue scales superlinearly as the number of books being borrowed \n",
    "increases:\n",
    "baseline = list_overdue_benchmark( 500)\n",
    "for count in (1_000, 1_500, 2_000):\n",
    "    comparison = list_overdue_benchmark(count)\n",
    "    print_delta(baseline, comparison)\n",
    ">>>\n",
    "Count   500 takes 0.001138s\n",
    " \n",
    "Count 1,000 takes 0.003317s\n",
    " 2.0x data size,  2.9x time\n",
    " \n",
    "Count 1,500 takes 0.007744s\n",
    " 3.0x data size,  6.8x time\n",
    " \n",
    "Count 2,000 takes 0.014739s\n",
    " 4.0x data size, 13.0x time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a208d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deque vs list pop(0) microbenchmark\n",
    "# Example: using deque for producer-consumer queues\n",
    "import collections\n",
    "from time import perf_counter\n",
    "\n",
    "def benchmark_pop0(count):\n",
    "    q = list(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.pop(0)\n",
    "    return perf_counter() - t0\n",
    "\n",
    "def benchmark_popleft(count):\n",
    "    q = collections.deque(range(count))\n",
    "    t0 = perf_counter()\n",
    "    while q:\n",
    "        q.popleft()\n",
    "    return perf_counter() - t0\n",
    "\n",
    "for c in (1000, 2000, 4000):\n",
    "    print(f'pop(0) for {c}:', benchmark_pop0(c))\n",
    "    print(f'popleft for {c}:', benchmark_popleft(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566699ad",
   "metadata": {},
   "source": [
    "## Item 73: Know How to Use heapq  for Priority Queues 339\n",
    "\n",
    "Item 73: Know How to Use heapq  for Priority Queues 339\n",
    "\n",
    "340 Chapter 8 Robustness and Performance\n",
    "When a book is returned before the due date, I need to do a linear \n",
    "scan in order to find the book in the queue and remove it. Removing \n",
    "a book causes all subsequent items in the list to be shifted back \n",
    "an index, which has a high cost that also scales superlinearly. Here, \n",
    "I define another micro-benchmark to test the performance of return-\n",
    "ing a book using this function:\n",
    "def list_return_benchmark(count):\n",
    "    def prepare():\n",
    "        queue = list(range(count))\n",
    "        random.shuffle(queue)\n",
    " \n",
    "        to_return = list(range(count))\n",
    "        random.shuffle(to_return)\n",
    " \n",
    "        return queue, to_return\n",
    " \n",
    "    def run(queue, to_return):\n",
    "        for i in to_return:\n",
    "            queue.remove(i)\n",
    " \n",
    "    tests = timeit.repeat(\n",
    "        setup ='queue, to_return = prepare()' ,\n",
    "        stmt =f'run(queue, to_return)' ,\n",
    "        globals =locals(),\n",
    "        repeat =100,\n",
    "        number =1)\n",
    " \n",
    "    return print_results(count, tests)\n",
    "And again, I can verify that indeed the performance degrades super-\n",
    "linearly as the number of books increases:\n",
    "baseline = list_return_benchmark( 500)\n",
    "for count in (1_000, 1_500, 2_000):\n",
    "    comparison = list_return_benchmark(count)\n",
    "    print_delta(baseline, comparison)\n",
    ">>>\n",
    "Count   500 takes 0.000898s\n",
    " \n",
    "Count 1,000 takes 0.003331s\n",
    " 2.0x data size,  3.7x time\n",
    " \n",
    "Count 1,500 takes 0.007674s\n",
    " 3.0x data size,  8.5x time\n",
    " \n",
    "\n",
    "Count 2,000 takes 0.013721s\n",
    " 4.0x data size, 15.3x time\n",
    "Using the methods of list may work for a tiny library, but it certainly \n",
    "won’t scale to the size of the Great Library of Alexandria, as I want it to!\n",
    "Fortunately, Python has the built-in heapq  m o d u l e  th a t  s o l v e s  th i s  \n",
    "problem by implementing priority queues efficiently. A heap is a data \n",
    "structure that allows for a list o f  i t e m s  t o  b e  m a i n t a i n e d  w h e r e  \n",
    "the computational complexity of adding a new item or removing the \n",
    "smallest item has logarithmic computational complexity (i.e., even \n",
    "better than linear scaling). In this library example, smallest means \n",
    "the book with the earliest due date. The best part about this module \n",
    "is that you don’t have to understand how heaps are implemented in \n",
    "order to use its functions correctly.\n",
    "Here, I reimplement the add_book  function using the heapq  module. \n",
    "The queue is still a plain list. The heappush  fun c ti o n  r e p l a c e s  th e  \n",
    "list.append  call from before. And I no longer have to call list.sort  on \n",
    "the queue:\n",
    "from heapq import heappush\n",
    " \n",
    "def add_book(queue, book):\n",
    "    heappush(queue, book)\n",
    "If I try to use this with the Book class as previously defined, I get this \n",
    "somewhat cryptic error:\n",
    "queue = []\n",
    "add_book(queue, Book( 'Little Women' , '2019-06-05' ))\n",
    "add_book(queue, Book( 'The Time Machine' , '2019-05-30' ))\n",
    ">>>\n",
    "Traceback ...\n",
    "TypeError: '<' not supported between instances of 'Book' and \n",
    "¯'Book'\n",
    "The heapq  module requires items in the priority queue to be compa-\n",
    "rable and have a natural sort order (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heapq priority queue example\n",
    "# Example: using heapq for priority queue\n",
    "import heapq\n",
    "class Book:\n",
    "    def __init__(self, title, due_date):\n",
    "        self.title = title\n",
    "        self.due_date = due_date\n",
    "    def __lt__(self, other):\n",
    "        # heapq is a min-heap; earlier due_date is higher priority\n",
    "        return self.due_date < other.due_date\n",
    "heap = []\n",
    "heapq.heappush(heap, Book('Don Quixote', '2019-06-07'))\n",
    "heapq.heappush(heap, Book('Frankenstein', '2019-06-05'))\n",
    "heapq.heappush(heap, Book('War and Peace', '2019-06-03'))\n",
    "while heap:\n",
    "    b = heapq.heappop(heap)\n",
    "    print('Next due:', b.title, b.due_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a44723",
   "metadata": {},
   "source": [
    "## Item 14: “Sort by Complex \n",
    "\n",
    "Item 14: “Sort by Complex \n",
    "Criteria Using the key Parameter” for details). You can quickly give \n",
    "the Book class this behavior by using the total_ordering  class dec-\n",
    "orator from the functools  built-in module (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc552e99",
   "metadata": {},
   "source": [
    "## Item 51: “Prefer Class \n",
    "\n",
    "Item 51: “Prefer Class \n",
    "Decorators Over Metaclasses for Composable Class Extensions” for \n",
    "background) and implementing the __lt__  special method (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31b164",
   "metadata": {},
   "source": [
    "## Item \n",
    "\n",
    "Item \n",
    "43: “Inherit from collections.abc  for Custom Container Types” for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d023ac",
   "metadata": {},
   "source": [
    "## Item 73: Know How to Use heapq  for Priority Queues 341\n",
    "\n",
    "Item 73: Know How to Use heapq  for Priority Queues 341\n",
    "\n",
    "342 Chapter 8 Robustness and Performance\n",
    "background). Here, I redefine the class with a less-than method that \n",
    "simply compares the due_date  fields between two Book instances:\n",
    "import functools\n",
    " \n",
    "@functools.total_ordering\n",
    "class Book:\n",
    "    def __init__(self, title, due_date):\n",
    "        self.title = title\n",
    "        self.due_date = due_date\n",
    " \n",
    "    def __lt__(self, other):\n",
    "        return self.due_date < other.due_date\n",
    "Now, I can add books to the priority queue by using the heapq.heappush  \n",
    "function without issues:\n",
    "queue = []\n",
    "add_book(queue, Book( 'Pride and Prejudice' , '2019-06-01' ))\n",
    "add_book(queue, Book( 'The Time Machine' , '2019-05-30' ))\n",
    "add_book(queue, Book( 'Crime and Punishment' , '2019-06-06' ))\n",
    "add_book(queue, Book( 'Wuthering Heights' , '2019-06-12' ))\n",
    "Alternatively, I can create a list with all of the books in any order and \n",
    "then use the sort method of list to produce the heap:\n",
    "queue = [\n",
    "    Book( 'Pride and Prejudice' , '2019-06-01' ),\n",
    "    Book( 'The Time Machine' , '2019-05-30' ),\n",
    "    Book( 'Crime and Punishment' , '2019-06-06' ),\n",
    "    Book( 'Wuthering Heights' , '2019-06-12' ),\n",
    "]\n",
    "queue.sort()\n",
    "Or I can use the heapq.heapify  fun c ti o n  to  crea t e  a  h ea p  in  lin ear  \n",
    "time (as opposed to the sort method’s len(queue) *  log(len(queue))  \n",
    "complexity):\n",
    "from heapq import heapify\n",
    " \n",
    "queue = [\n",
    "    Book( 'Pride and Prejudice' , '2019-06-01' ),\n",
    "    Book( 'The Time Machine' , '2019-05-30' ),\n",
    "    Book( 'Crime and Punishment' , '2019-06-06' ),\n",
    "    Book( 'Wuthering Heights' , '2019-06-12' ),\n",
    "]\n",
    "heapify(queue)\n",
    "\n",
    "To check for overdue books, I inspect the first element in the list \n",
    "instead of the last, and then I use the heapq.heappop  function instead \n",
    "of the list.pop  function:\n",
    "from heapq import heappop\n",
    " \n",
    "def next_overdue_book(queue, now):\n",
    "    if queue:\n",
    "        book = queue[0]           # Most overdue first\n",
    "        if book.due_date < now:\n",
    "            heappop(queue)        # Remove the overdue book\n",
    "            return book\n",
    " \n",
    "    raise NoOverdueBooks\n",
    "Now, I can find and remove overdue books in order until there are \n",
    "none left for the current time:\n",
    "now = '2019-06-02'\n",
    " \n",
    "book = next_overdue_book(queue, now)\n",
    "print(book.title)\n",
    " \n",
    "book = next_overdue_book(queue, now)\n",
    "print(book.title)\n",
    " \n",
    "try:\n",
    "    next_overdue_book(queue, now)\n",
    "except NoOverdueBooks:\n",
    "    pass          # Expected\n",
    "else:\n",
    "    assert False  # Doesn't happen\n",
    ">>>\n",
    "The Time Machine\n",
    "Pride and Prejudice\n",
    "I can write another micro-benchmark to test the performance of this \n",
    "implementation that uses the heapq  module:\n",
    "def heap_overdue_benchmark(count):\n",
    "    def prepare():\n",
    "        to_add = list(range(count))\n",
    "        random.shuffle(to_add)\n",
    "        return [], to_add\n",
    " \n",
    "    def run(queue, to_add):\n",
    "        for i in to_add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heapq priority queue example\n",
    "# Example: using heapq for priority queue\n",
    "import heapq\n",
    "class Book:\n",
    "    def __init__(self, title, due_date):\n",
    "        self.title = title\n",
    "        self.due_date = due_date\n",
    "    def __lt__(self, other):\n",
    "        # heapq is a min-heap; earlier due_date is higher priority\n",
    "        return self.due_date < other.due_date\n",
    "heap = []\n",
    "heapq.heappush(heap, Book('Don Quixote', '2019-06-07'))\n",
    "heapq.heappush(heap, Book('Frankenstein', '2019-06-05'))\n",
    "heapq.heappush(heap, Book('War and Peace', '2019-06-03'))\n",
    "while heap:\n",
    "    b = heapq.heappop(heap)\n",
    "    print('Next due:', b.title, b.due_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b08bf",
   "metadata": {},
   "source": [
    "## Item 73: Know How to Use heapq  for Priority Queues 343\n",
    "\n",
    "Item 73: Know How to Use heapq  for Priority Queues 343\n",
    "\n",
    "344 Chapter 8 Robustness and Performance\n",
    "            heappush(queue, i)\n",
    "        while queue:\n",
    "            heappop(queue)\n",
    " \n",
    "    tests = timeit.repeat(\n",
    "        setup ='queue, to_add = prepare()' ,\n",
    "        stmt =f'run(queue, to_add)' ,\n",
    "        globals =locals(),\n",
    "        repeat =100,\n",
    "        number =1)\n",
    " \n",
    "    return print_results(count, tests)\n",
    "This benchmark experimentally verifies that the heap-based \n",
    "priority queue implementation scales much better (roughly \n",
    "len(queue) * math.log(len(queue)) ), without superlinearly degrading \n",
    "performance:\n",
    "baseline = heap_overdue_benchmark( 500)\n",
    "for count in (1_000, 1_500, 2_000):\n",
    "    comparison = heap_overdue_benchmark(count)\n",
    "    print_delta(baseline, comparison)\n",
    ">>>\n",
    "Count   500 takes 0.000150s\n",
    " \n",
    "Count 1,000 takes 0.000325s\n",
    " 2.0x data size,  2.2x time\n",
    " \n",
    "Count 1,500 takes 0.000528s\n",
    " 3.0x data size,  3.5x time\n",
    " \n",
    "Count 2,000 takes 0.000658s\n",
    " 4.0x data size,  4.4x time\n",
    "With the heapq  implementation, one question remains: How should \n",
    "I handle returns that are on time? The solution is to never remove a \n",
    "book from the priority queue until its due date. At that time, it will \n",
    "be the first item in the list, and I can simply ignore the book if it’s \n",
    "already been returned. Here, I implement this behavior by adding a \n",
    "new field to track the book’s return status:\n",
    "@functools.total_ordering\n",
    "class Book:\n",
    "    def __init__(self, title, due_date):\n",
    "        self.title = title\n",
    "        self.due_date = due_date\n",
    "\n",
    "        self.returned = False  # New field\n",
    " \n",
    "    ...\n",
    "Then, I change the next_overdue_book  function to repeatedly ignore \n",
    "any book that’s already been returned:\n",
    "def next_overdue_book(queue, now):\n",
    "    while queue:\n",
    "        book = queue[0]\n",
    "        if book.returned:\n",
    "            heappop(queue)\n",
    "            continue\n",
    " \n",
    "        if book.due_date < now:\n",
    "            heappop(queue)\n",
    "            return book\n",
    " \n",
    "        break\n",
    " \n",
    "    raise NoOverdueBooks\n",
    "This approach makes the return_book  function extremely fast \n",
    "because it makes no modifications to the priority queue:\n",
    "def return_book(queue, book):\n",
    "    book.returned = True\n",
    "The downside of this solution for returns is that the priority queue \n",
    "may grow to the maximum size it would have needed if all books from \n",
    "the library were checked out and went overdue. Although the queue \n",
    "o p e ra ti o n s  will  b e  f as t  th anks  t o  heapq , this storage overhead may \n",
    "take significant memory (see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1bd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heapq priority queue example\n",
    "# Example: using heapq for priority queue\n",
    "import heapq\n",
    "class Book:\n",
    "    def __init__(self, title, due_date):\n",
    "        self.title = title\n",
    "        self.due_date = due_date\n",
    "    def __lt__(self, other):\n",
    "        # heapq is a min-heap; earlier due_date is higher priority\n",
    "        return self.due_date < other.due_date\n",
    "heap = []\n",
    "heapq.heappush(heap, Book('Don Quixote', '2019-06-07'))\n",
    "heapq.heappush(heap, Book('Frankenstein', '2019-06-05'))\n",
    "heapq.heappush(heap, Book('War and Peace', '2019-06-03'))\n",
    "while heap:\n",
    "    b = heapq.heappop(heap)\n",
    "    print('Next due:', b.title, b.due_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d32bb",
   "metadata": {},
   "source": [
    "## Item 81: “Use tracemalloc  to Understand \n",
    "\n",
    "Item 81: “Use tracemalloc  to Understand \n",
    "Memory Usage and Leaks” for how to debug such usage).\n",
    "That said, if you’re trying to build a robust system, you need to plan \n",
    "for the worst-case scenario; thus, you should expect that it’s possible \n",
    "for every library book to go overdue for some reason (e.g., a natural \n",
    "disaster closes the road to the library). This memory cost is a design \n",
    "consideration that you should have already planned for and mitigated \n",
    "through additional constraints (e.g., imposing a maximum number of \n",
    "simultaneously lent books).\n",
    "Beyond the priority queue primitives that I’ve used in this example, \n",
    "the heapq  module provides additional functionality for advanced use \n",
    "cases (see help(heapq) ). The module is a great choice when its function-\n",
    "ality matches the problem you’re facing (see the queue.PriorityQueue  \n",
    "class for another thread-safe option)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3785d25",
   "metadata": {},
   "source": [
    "## Item 73: Know How to Use heapq  for Priority Queues 345\n",
    "\n",
    "Item 73: Know How to Use heapq  for Priority Queues 345\n",
    "\n",
    "346 Chapter 8 Robustness and Performance\n",
    "Things to Remember\n",
    "✦ Priority queues allow you to process items in order of importance \n",
    "instead of in first-in, first-out order.\n",
    "✦ If you try to use list operations to implement a priority queue, your \n",
    "program’s performance will degrade superlinearly as the queue \n",
    "grows.\n",
    "✦ The heapq  built-in module provides all of the functions you need to \n",
    "implement a priority queue that scales efficiently.\n",
    "✦ To use heapq , the items being prioritized must have a natural sort \n",
    "order, which requires special methods like __lt__  to be defined for \n",
    "classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80de09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heapq priority queue example\n",
    "# Example: using heapq for priority queue\n",
    "import heapq\n",
    "class Book:\n",
    "    def __init__(self, title, due_date):\n",
    "        self.title = title\n",
    "        self.due_date = due_date\n",
    "    def __lt__(self, other):\n",
    "        # heapq is a min-heap; earlier due_date is higher priority\n",
    "        return self.due_date < other.due_date\n",
    "heap = []\n",
    "heapq.heappush(heap, Book('Don Quixote', '2019-06-07'))\n",
    "heapq.heappush(heap, Book('Frankenstein', '2019-06-05'))\n",
    "heapq.heappush(heap, Book('War and Peace', '2019-06-03'))\n",
    "while heap:\n",
    "    b = heapq.heappop(heap)\n",
    "    print('Next due:', b.title, b.due_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63042e",
   "metadata": {},
   "source": [
    "## Item 74:  Consider memoryview  and bytearray  for \n",
    "\n",
    "Item 74:  Consider memoryview  and bytearray  for \n",
    "Zero-Copy Interactions with bytes\n",
    "Although Python isn’t able to parallelize CPU-bound computation \n",
    "without extra effort (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3673f",
   "metadata": {},
   "source": [
    "## Item 64: “Consider concurrent.futures  for \n",
    "\n",
    "Item 64: “Consider concurrent.futures  for \n",
    "True Parallelism”), it is able to support high-throughput, parallel I/O \n",
    "in a variety of ways (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50811e",
   "metadata": {},
   "source": [
    "## Item 53: “Use Threads for Blocking I/O, Avoid \n",
    "\n",
    "Item 53: “Use Threads for Blocking I/O, Avoid \n",
    "for Parallelism” and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771486dd",
   "metadata": {},
   "source": [
    "## Item 60: “Achieve Highly Concurrent I/O with \n",
    "\n",
    "Item 60: “Achieve Highly Concurrent I/O with \n",
    "Coroutines”). That said, it’s surprisingly easy to use these I/O tools \n",
    "the wrong way and reach the conclusion that the language is too slow \n",
    "for even I/O-bound workloads.\n",
    "For example, say that I’m building a media server to stream television \n",
    "or movies over a network to users so they can watch without having \n",
    "to download the video data in advance. One of the key features of \n",
    "such a system is the ability for users to move forward or backward \n",
    "in the video playback so they can skip or repeat parts. In the client \n",
    "program, I can implement this by requesting a chunk of data from the \n",
    "server corresponding to the new time index selected by the user:\n",
    "def timecode_to_index(video_id, timecode):\n",
    "    ...\n",
    "    # Returns the byte offset in the video data\n",
    " \n",
    "def request_chunk(video_id, byte_offset, size):\n",
    "    ...\n",
    "    # Returns size bytes of video_id's data from the offset\n",
    " \n",
    "video_id = ...\n",
    "timecode = '01:09:14:28'\n",
    "byte_offset = timecode_to_index(video_id, timecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5978b1e",
   "metadata": {},
   "source": [
    "## Item 74: Consider memoryview  for zero-copy interactions 347\n",
    "\n",
    "Item 74: Consider memoryview  for zero-copy interactions 347\n",
    "size = 20 * 1024 * 1024\n",
    "video_data = request_chunk(video_id, byte_offset, size)\n",
    "How would you implement the server-side handler that receives the \n",
    "request_chunk  request and returns the corresponding 20 MB chunk \n",
    "of video data? For the sake of this example, I assume that the com-\n",
    "mand and control parts of the server have already been hooked up \n",
    "(see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95bafb",
   "metadata": {},
   "source": [
    "## Item 61: “Know How to Port Threaded I/O to asyncio ” for what \n",
    "\n",
    "Item 61: “Know How to Port Threaded I/O to asyncio ” for what \n",
    "that requires). I focus here on the last steps where the requested \n",
    "chunk is extracted from gigabytes of video data that’s cached in mem-\n",
    "ory and is then sent over a socket back to the client. Here’s what the \n",
    "implementation would look like:\n",
    "socket = ...             # socket connection to client\n",
    "video_data = ...         # bytes containing data for video_id\n",
    "byte_offset = ...        # Requested starting position\n",
    "size = 20 * 1024 * 1024  # Requested chunk size\n",
    " \n",
    "chunk = video_data[byte_offset:byte_offset + size]\n",
    "socket.send(chunk)\n",
    "The latency and throughput of this code will come down to two fac-\n",
    "tors: how much time it takes to slice the 20 MB video chunk from \n",
    "video_data , and how much time the socket takes to transmit that \n",
    "data to the client. If I assume that the socket is infinitely fast, I can \n",
    "run a micro-benchmark by using the timeit  built-in module to under-\n",
    "stand the performance characteristics of slicing bytes  instances this \n",
    "way to create chunks (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f16c75",
   "metadata": {},
   "source": [
    "## Item 11: “Know How to Slice Sequences” for \n",
    "\n",
    "Item 11: “Know How to Slice Sequences” for \n",
    "background):\n",
    "import timeit\n",
    " \n",
    "def run_test():\n",
    "    chunk = video_data[byte_offset:byte_offset + size]\n",
    "    # Call socket.send(chunk), but ignoring for benchmark\n",
    " \n",
    "result = timeit.timeit(\n",
    "    stmt ='run_test()' ,\n",
    "    globals =globals(),\n",
    "    number =100) / 100\n",
    " \n",
    "print(f'{result:0.9f} seconds' )\n",
    ">>>\n",
    "0.004925669 seconds\n",
    "\n",
    "348 Chapter 8 Robustness and Performance\n",
    "It took roughly 5 milliseconds to extract the 20 MB slice of data to \n",
    "transmit to the client. That means the overall throughput of my \n",
    "server is limited to a theoretical maximum of 20 MB / 5 milliseconds \n",
    "= 7.3 GB / second, since that’s the fastest I can extract the video \n",
    "data from memory. My server will also be limited to 1 CPU-second / \n",
    "5  milliseconds = 200 clients requesting new chunks in parallel, which \n",
    "is tiny compared to the tens of thousands of simultaneous connec-\n",
    "tions that tools like the asyncio  b u i l t - i n  m o d u l e  c a n  s u p p o r t .  T h e  \n",
    "problem is that slicing a bytes  instance causes the underlying data to \n",
    "be copied, which takes CPU time.\n",
    "A better way to write this code is by using Python’s built-in memoryview  \n",
    "type, which exposes CPython’s high-performance buffer protocol  to \n",
    "programs. The buffer protocol is a low-level C API that allows the \n",
    "Python runtime and C extensions to access the underlying data \n",
    "buffers that are behind objects like bytes  instances. The best part \n",
    "about memoryview  instances is that slicing them results in another \n",
    "memoryview  instance without copying the underlying data. Here, I cre-\n",
    "ate a memoryview  wrapping a bytes  instance and inspect a slice of it:\n",
    "data = b'shave and a haircut, two bits'\n",
    "view = memoryview (data)\n",
    "chunk = view[12:19]\n",
    "print(chunk)\n",
    "print('Size:           ' , chunk.nbytes)\n",
    "print('Data in view:   ' , chunk.tobytes())\n",
    "print('Underlying data:' , chunk.obj)\n",
    ">>>\n",
    "<memory at 0x10951fb80>\n",
    "Size:            7\n",
    "Data in view:    b'haircut'\n",
    "Underlying data: b'shave and a haircut, two bits'\n",
    "By enabling zero-copy  operations, memoryview  can provide enor-\n",
    "mous speedups for code that needs to quickly process large amounts \n",
    "of memory, such as numerical C extensions like NumPy and \n",
    "I/O-bound programs like this one. Here, I replace the simple bytes  \n",
    "slicing from above with memoryview  slicing instead and repeat the \n",
    "same micro-benchmark:\n",
    "video_view = memoryview (video_data)\n",
    " \n",
    "def run_test():\n",
    "    chunk = video_view[byte_offset:byte_offset + size]\n",
    "    # Call socket.send(chunk), but ignoring for benchmark\n",
    " \n",
    "\n",
    "result = timeit.timeit(\n",
    "    stmt ='run_test()' ,\n",
    "    globals =globals(),\n",
    "    number =100) / 100\n",
    " \n",
    "print(f'{result:0.9f} seconds' )\n",
    ">>>\n",
    "0.000000250 seconds\n",
    "The result is 250 nanoseconds. Now the theoretical maximum through-\n",
    "put of my server is 20 MB / 250 nanoseconds = 164 TB / second. \n",
    "For parallel clients, I can theoretically support up to 1 CPU- second / \n",
    "250 nanoseconds = 4 million. That’s more like it! This means that \n",
    "now my program is entirely bound by the underlying performance of \n",
    "the socket connection to the client, not by CPU constraints.\n",
    "Now, imagine that the data must flow in the other direction, where \n",
    "some clients are sending live video streams to the server in order to \n",
    "broadcast them to other users. In order to do this, I need to store the \n",
    "latest video data from the user in a cache that other clients can read \n",
    "from. Here’s what the implementation of reading 1 MB of new data \n",
    "from the incoming client would look like:\n",
    "socket = ...        # socket connection to the client\n",
    "video_cache = ...   # Cache of incoming video stream\n",
    "byte_offset = ...   # Incoming buffer position\n",
    "size = 1024 * 1024  # Incoming chunk size\n",
    " \n",
    "chunk = socket.recv(size)\n",
    "video_view = memoryview (video_cache)\n",
    "before = video_view[:byte_offset]\n",
    "after = video_view[byte_offset + size:]\n",
    "new_cache = b''.join([before, chunk, after])\n",
    "The socket.recv  method returns a bytes  instance. I can splice the \n",
    "new data with the existing cache at the current byte_offset  by using \n",
    "simple slicing operations and the bytes.join  method. To understand \n",
    "the performance of this, I can run another micro-benchmark. I’m \n",
    "using a dummy socket, so the performance test is only for the mem-\n",
    "ory operations, not the I/O interaction:\n",
    "def run_test():\n",
    "    chunk = socket.recv(size)\n",
    "    before = video_view[:byte_offset]\n",
    "    after = video_view[byte_offset + size:]\n",
    "    new_cache = b''.join([before, chunk, after])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca9275",
   "metadata": {},
   "source": [
    "## Item 74: Consider memoryview  for zero-copy interactions 349\n",
    "\n",
    "Item 74: Consider memoryview  for zero-copy interactions 349\n",
    "\n",
    "350 Chapter 8 Robustness and Performance\n",
    "result = timeit.timeit(\n",
    "    stmt ='run_test()' ,\n",
    "    globals =globals(),\n",
    "    number =100) / 100\n",
    " \n",
    "print(f'{result:0.9f} seconds' )\n",
    ">>>\n",
    "0.033520550 seconds\n",
    "It takes 33 milliseconds to receive 1 MB and update the video cache. \n",
    "This means my maximum receive throughput is 1 MB / 33  milliseconds \n",
    "= 31 MB / second, and I’m limited to 31 MB / 1 MB = 31 simultaneous \n",
    "clients streaming in video data this way. This doesn’t scale.\n",
    "A better way to write this code is to use Python’s built-in bytearray  \n",
    "type in conjunction with memoryview . One limitation with bytes  \n",
    "instances is that they are read-only and don’t allow for individual \n",
    "indexes to be updated:\n",
    "my_bytes = b'hello'\n",
    "my_bytes[ 0] = b'\\x79'\n",
    ">>>\n",
    "Traceback ...\n",
    "TypeError: 'bytes' object does not support item assignment\n",
    "The bytearray  type is like a mutable version of bytes  that allows for \n",
    "arbitrary positions to be overwritten. bytearray  uses integers for its \n",
    "values instead of bytes :\n",
    "my_array = bytearray (b'hello' )\n",
    "my_array[ 0] = 0x79\n",
    "print(my_array)\n",
    ">>>\n",
    "bytearray(b'yello')\n",
    "A memoryview  can also be used to wrap a bytearray . When you slice \n",
    "such a memoryview , the resulting object can be used to assign data to a \n",
    "particular portion of the underlying buffer. This eliminates the copy-\n",
    "ing costs from above that were required to splice the bytes  instances \n",
    "back together after data was received from the client:\n",
    "my_array = bytearray (b'row, row, row your boat' )\n",
    "my_view = memoryview (my_array)\n",
    "write_view = my_view[ 3:13]\n",
    "write_view[:] = b'-10 bytes-'\n",
    "print(my_array)\n",
    "\n",
    ">>>\n",
    "bytearray(b'row-10 bytes- your boat')\n",
    "Many library methods in Python, such as socket.recv_into  and \n",
    "RawIOBase.readinto , use the buffer protocol to receive or read data \n",
    "quickly. The benefit of these methods is that they avoid allocating \n",
    "memory and creating another copy of the data; what’s received goes \n",
    "straight into an existing buffer. Here, I use socket.recv_into  along \n",
    "with a memoryview  slice to receive data into an underlying bytearray  \n",
    "without the need for splicing:\n",
    "video_array = bytearray (video_cache)\n",
    "write_view = memoryview (video_array)\n",
    "chunk = write_view[byte_offset:byte_offset + size]\n",
    "socket.recv_into(chunk)\n",
    "I can run another micro-benchmark to compare the performance of \n",
    "this approach to the earlier example that used socket.recv :\n",
    "def run_test():\n",
    "    chunk = write_view[byte_offset:byte_offset + size]\n",
    "    socket.recv_into(chunk)\n",
    " \n",
    "result = timeit.timeit(\n",
    "    stmt ='run_test()' ,\n",
    "    globals =globals(),\n",
    "    number =100) / 100\n",
    " \n",
    "print(f'{result:0.9f} seconds' )\n",
    ">>>\n",
    "0.000033925 seconds\n",
    "It took 33 microseconds to receive a 1 MB video transmission. This \n",
    "means my server can support 1 MB / 33 microseconds = 31 GB /  \n",
    "second of max throughput, and 31 GB / 1 MB = 31,000 parallel \n",
    "streaming clients. That’s the type of scalability that I’m looking for!\n",
    "Things to Remember\n",
    "✦ The memoryview  built-in type provides a zero-copy interface for \n",
    "reading and writing slices of objects that support Python’s high- \n",
    "performance buffer protocol.\n",
    "✦ The bytearray  b u i l t - i n  t y p e  p r o v i d e s  a  m u t a b l e  bytes -like type \n",
    "that can be used for zero-copy data reads with functions like \n",
    "socket.recv_from .\n",
    "✦ A memoryview  can wrap a bytearray , allowing for received data to be \n",
    "spliced into an arbitrary buffer location without copying costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d2b3c3",
   "metadata": {},
   "source": [
    "## Item 74: Consider memoryview  for zero-copy interactions 351\n",
    "\n",
    "Item 74: Consider memoryview  for zero-copy interactions 351\n",
    "\n",
    "This page intentionally left blank \n",
    "\n",
    "9Testing and \n",
    "Debugging\n",
    "Python doesn’t have compile-time static type checking. There’s \n",
    " nothing in the interpreter that will ensure that your program will \n",
    "work correctly when you run it. Python does support optional type \n",
    "annotations that can be used in static analysis to detect many kinds \n",
    "of bugs (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb3d9b",
   "metadata": {},
   "source": [
    "## Item 90: “Consider Static Analysis via typing  to Obviate \n",
    "\n",
    "Item 90: “Consider Static Analysis via typing  to Obviate \n",
    "Bugs” for details). However, it’s still fundamentally a dynamic lan-\n",
    "guage, and anything is possible. With Python, you ultimately don’t \n",
    "know if the functions your program calls will be defined at runtime, \n",
    "even when their existence is evident in the source code. This dynamic \n",
    "behavior is both a blessing and a curse.\n",
    "The large numbers of Python programmers out there say it’s worth \n",
    "going without compile-time static type checking because of the pro-\n",
    "ductivity gained from the resulting brevity and simplicity. But most \n",
    "people using Python have at least one horror story about a program \n",
    "encountering a boneheaded error at runtime. One of the worst exam-\n",
    "ples I’ve heard of involved a SyntaxError  being raised in production as \n",
    "a side effect of a dynamic import (see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec478ca",
   "metadata": {},
   "source": [
    "## Item 88: “Know How to Break \n",
    "\n",
    "Item 88: “Know How to Break \n",
    "Circular Dependencies”), resulting in a crashed server process. The \n",
    "programmer I know who was hit by this surprising occurrence has \n",
    "since ruled out using Python ever again.\n",
    "But I have to wonder, why wasn’t the code more well tested before \n",
    "the program was deployed to production? Compile-time static type \n",
    "safety isn’t everything. You should always test your code, regardless \n",
    "of what language it’s written in. However, I’ll admit that in Python it \n",
    "may be more important to write tests to verify correctness than in \n",
    "other languages. Luckily, the same dynamic features that create risks \n",
    "also make it extremely easy to write tests for your code and to debug \n",
    "malfunctioning programs. You can use Python’s dynamic nature and \n",
    "easily overridable behaviors to implement tests and ensure that your \n",
    "programs work as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec1b05",
   "metadata": {},
   "source": [
    "## Extra real-world examples (added)\n",
    "These examples were added on request for clarity and practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8741aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra: robust file-read helper with retries (practical pattern)\n",
    "import time\n",
    "import random\n",
    "\n",
    "def robust_read(path, attempts=3, backoff=0.5):\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except (OSError, UnicodeDecodeError) as e:\n",
    "            if i == attempts - 1:\n",
    "                raise\n",
    "            sleep = backoff * (2 ** i) + random.random() * 0.1\n",
    "            time.sleep(sleep)\n",
    "\n",
    "# Demonstration: write then read\n",
    "p = '/mnt/data/_robust.txt'\n",
    "with open(p, 'w', encoding='utf-8') as f:\n",
    "    f.write('hello robust')\n",
    "print('read:', robust_read(p))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
